{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKZw2LJam24X"
      },
      "source": [
        "## IMPORTING LIBRARIES AND DATASET DOWNLOAD AND PRETRAINED MODELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wj-OYlly7wA4",
        "outputId": "416a9a16-4cba-471b-da53-aa30a541bafc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# System Libraries\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "\n",
        "# Loop libraries\n",
        "from tqdm import tqdm\n",
        "from itertools import product\n",
        "\n",
        "# Handling Data Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# display libraries\n",
        "from IPython.display import Audio, clear_output\n",
        "import warnings\n",
        "\n",
        "# Plot Libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import plotly.express as px\n",
        "import seaborn as sns\n",
        "\n",
        "# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files.\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "# text handling libraries\n",
        "import re\n",
        "\n",
        "# ML libraries\n",
        "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# DL libraries\n",
        "import tensorflow as tf\n",
        "import keras as ks\n",
        "\n",
        "EMO     = ['Anger', 'Happiness', 'Sadness', 'Neutral', 'Frustration', 'Excited', 'Fear', 'Surprise', 'Disgust', 'Other']\n",
        "EMO_SIX = ['neu', 'fru', 'ang', 'sad', 'hap', 'exc']\n",
        "VAD     = ['valence', 'activation', 'dominance']\n",
        "TAG_ID  = ['session_id', 'conv_id', 'turn_id']\n",
        "\n",
        "label2id = {'neu': 0, 'fru': 1, 'ang': 2, 'sad': 3, 'hap': 4, 'exc': 5, 'dis': 6, 'fea': 7, 'oth': 8, 'sur': 9, 'xxx': 10}\n",
        "\n",
        "# setting a seed (will be later used as input to other funcs)\n",
        "random_seed = 42\n",
        "\n",
        "# printing numpy arrays on single lines\n",
        "np.set_printoptions(linewidth=np.inf)\n",
        "\n",
        "# Decide or not if you want to save the results on your drive by changing to True use_drive\n",
        "\n",
        "use_drive = True\n",
        "if use_drive:\n",
        "    from google.colab import drive\n",
        "    drive_folder = os.path.join(os.getcwd(), 'gdrive')\n",
        "    drive.mount(drive_folder)\n",
        "    new_drive_folder = 'NLP_Models'\n",
        "    models_folder = os.path.join(drive_folder, 'MyDrive', new_drive_folder)\n",
        "    if not os.path.exists(models_folder):\n",
        "        os.makedirs(models_folder)\n",
        "        print(f\"{new_drive_folder} folder is created in your drive!\")\n",
        "    new_drive_folder_logs = 'NLP_models_ArchAndWeights'\n",
        "    models_folder_logs = os.path.join(drive_folder, 'MyDrive', new_drive_folder_logs)\n",
        "    if not os.path.exists(models_folder_logs):\n",
        "        os.makedirs(models_folder_logs)\n",
        "        print(f\"{new_drive_folder_logs} folder is created in your drive!\")\n",
        "else:\n",
        "    models_folder = os.path.join(os.getcwd(), 'NLP_Models')\n",
        "    if not os.path.exists(models_folder):\n",
        "        os.makedirs(models_folder)\n",
        "        print(f\"{models_folder} folder is created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQm-pveQoMO1"
      },
      "source": [
        "The dataset is distributed on this webpage https://sail.usc.edu/iemocap/ under request, after the request is accepted they provide you a download link that expire in 48 hours therefore we uploaded a lighter copy of it on kaggle as a private dataset (only the files we were interested in), therefore to run the following cell you just have to ask the access to Riccardo Paolini, we will provide you the access as soon as possible, then you have to download a ***kaggle.json*** file from kaggle to do that follow this steps:\n",
        "- go to https://www.kaggle.com/\n",
        "- sign up/in to your account on kaggle\n",
        "- click on your propic in the top right corner\n",
        "- click \"Your Profile -> Account -> Create New API Token\"\n",
        "\n",
        "after this steps the download of your ***kaggle.json*** file should have started.\n",
        "By running the following cell you are requested to upload the ***kaggle.json*** therefore the download of the private dataset should start."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BEJaD6T7sHs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c826482-8b65-40a7-db9a-1b52dd05f62c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 73 Jul  2 14:01 kaggle.json\n",
            "Downloading nlp-project-work.zip to /content\n",
            " 37% 399M/1.04G [00:14<00:21, 32.0MB/s]"
          ]
        }
      ],
      "source": [
        "if use_drive:\n",
        "  from google.colab import files\n",
        "  #files.upload()\n",
        "  !echo '{\"username\":\"ekinderpeskinder\",\"key\":\"7bf2fa567986da7324e3a29c6519959a\"}' | cat > kaggle.json\n",
        "  !ls -lha kaggle.json\n",
        "  !pip install -q kaggle # Install kaggle API\n",
        "  !mkdir -p ~/.kaggle\n",
        "  !cp kaggle.json ~/.kaggle/\n",
        "  !chmod 600 /root/.kaggle/kaggle.json\n",
        "  !kaggle datasets download -d riccardopaolini/nlp-project-work\n",
        "  !unzip nlp-project-work.zip\n",
        "  clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WNAxzFN94N-"
      },
      "outputs": [],
      "source": [
        "use_text_transformers = False\n",
        "\n",
        "transformer_name = \"albert-base-v2\"\n",
        "\n",
        "if use_text_transformers:\n",
        "    ! pip install transformers\n",
        "    from transformers import AutoTokenizer, TFAutoModel\n",
        "\n",
        "    text_transformer_tokenizer = AutoTokenizer.from_pretrained(transformer_name)\n",
        "    text_transformer = TFAutoModel.from_pretrained(transformer_name)\n",
        "\n",
        "    max_len_text_transformer = text_transformer.config.max_position_embeddings # THIS IS THE TRUE MAX_LEN OF EMO_ROBERTA\n",
        "    text_transformer_tokenizer_params = {\n",
        "        'padding': 'max_length', # pads to the max_length if given otherwise pads to max_lenght of the model\n",
        "        'truncation': True,\n",
        "        'max_length': max_len_text_transformer,\n",
        "        'return_tensors': 'np'\n",
        "    }\n",
        "    clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5nZSJzS0UdA"
      },
      "outputs": [],
      "source": [
        "sample_rate = 16000\n",
        "use_audio_transformers = False\n",
        "\n",
        "if use_audio_transformers:\n",
        "    ! pip install transformers\n",
        "\n",
        "    from transformers import TFWav2Vec2Model, AutoFeatureExtractor\n",
        "\n",
        "    audio_transformer_name = \"facebook/wav2vec2-base\"  # Name of pretrained model from Hugging Face Model Hub\n",
        "\n",
        "    wav2vec2 = TFWav2Vec2Model.from_pretrained(audio_transformer_name, apply_spec_augment=False, from_pt=True)\n",
        "    feature_extractor = AutoFeatureExtractor.from_pretrained(audio_transformer_name, return_attention_mask=True)\n",
        "\n",
        "    clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1bxMwrKkhfp"
      },
      "source": [
        "## DATASET DESCRIPTION (Copy Paste of the README.txt file of the dataset)\n",
        "\n",
        "Thank you for your interest in the USC_IEMOCAP database.  The USC_IEMOCAP database contains audio, transcriptions, video, and motion-capture(MoCap) recordings of dyadic mixed-gender pairs of actors.  There are five sessions in the database (ten actors total). For complete information about IEMOCAP please refer to:\n",
        "\n",
        "Busso, Carlos et al.  \"IEMOCAP: Interactive emotional dyadic motion capture database.\"  Journal of Language Resources and Evaluation.  Volume 42, Number 4.  pp. 335-359.  2008\n",
        "\n",
        "as well as our website: http://sail.usc.edu/iemocap/\n",
        "\n",
        "The recorded dialogs are either improvisations of affective scenarios, or performances of theatrical scripts. They have been manually segmented into utterances. Each utterance from either of the actors in the interaction has been evaluated categorically over the set of: {angry, happy, sad, neutral, frustrated, excited, fearful, surprised, disgusted, other} by at least three different annotators, and dimensionally over the axes of: valence (positive vs. negative); activation (calm vs. excited); and dominance (passive vs. aggressive) by at least two different annotators.\n",
        "\n",
        "In each recording of a session only one actor wears MoCap markers while both are being recorded by microphones and cameras. Thus there are available MoCap data (facial expression, head and hand movement) for one actor per recording, while there are wavefile and videos for both actors. The naming convention regarding the data is e.g., Ses01F_impro01 while indicates Session1, where the Female actor is wearing the markers and actors are performing improvisation 1. The release contains two formats: dialog format which contains data from the entire dyadic interaction and the sentence format where the data per dialog (recording) have been further segmented into utterances (see folders SessionX/dialog and SessionX/sentences respectively). For the utterance format the naming is as follows: Ses01F_impro01_M000 indicates first session, Female actor is wearing markers, actors are performing improvisation 1 and this is the first utterance of the Male actor. The timing of the sentences in each dialog can be found in the lab files in SessionX/dialog/lab\n",
        "\n",
        "The MoCap data can be found in either dialog format or sentece format in the corresposding folders. The MOCAP_rotated folder contains motion-capture data rotated and translated such that the central nose marker always represents the (0,0,0) xyz-tuple. The MoCap data are in .txt files where each line is a frame number and each column is a marker coordinate. Please consult the headers of the MoCap files and the diagram of the facial markers (Documentation/FIVE_face_markers2.png) to see the correspondence between marker positions and marker names. The hand mocap data is in MOCAP_hand folder, while the information of head translation (x,y,z) and head rotation (yaw, pitch, roll) is in MOCAP_head folder.\n",
        "\n",
        "We are also distributing wavefiles (sentence and dialog format), the videos of the recordings (SessionX/dialog/avi/), and transcriptions of the dialogs (SessionX/dialog/transcriptions/). For the segmented utterances we are also providing the results of forced alignment which contain detail phoneme, syllable and word level timing information (SessionX/sentences/ForcedAlignment/).\n",
        "\n",
        "The evaluations (emotional annotations) for each recording and each uterance are contained in folder SessionX/dialog/Evaluation/. Each file provides the detailed evaluation reports for the categorical evaluators (e.g., C-E1), the dimensional evaluators (e.g., A-E1), and the self-evaluatiors (e.g., C-F1 or C-M1, A-F1 or A-M1). The utterance-level information can be found in the first line of an utterance summary.  The first entry represents the start and end times for the utterance.  The second entry is the utterance name (e.g., Ses01_impro01_F003).  The third entry is the ground truth (if no majority ground truth could be assigned, the ground truth is labeled xxx).  The final engry is the average dimensional evaluation (over the evaluators, except the self-evaluators).\n",
        "\n",
        "We are providing  a full release of this data in the hopes that it will provide a valuable resource to the emotion recognition community. We request that any published work using IEMOCAP should cite the paper entitled: \"IEMOCAP: Interactive emotional dyadic motion capture database\" (JLRE, 2008). If you do any further evaluation of the data, we request that you send us the detailed results so that we may provide a more detailed resource to the community. Also, please feel free to send us feedback regarding the database: how it is being used, if the information provided is sufficient, and how you have decided to utilize the evaluation information. Thanks again.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbxy9wE_nQLR"
      },
      "source": [
        "## BUILDING OF A DATAFRAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mfn3y7jFJ7Hu"
      },
      "outputs": [],
      "source": [
        "folder = os.path.join(os.getcwd(), 'IEMOCAP')\n",
        "\n",
        "conv_id = 0\n",
        "\n",
        "df = []\n",
        "for session in ['Session1','Session2','Session3','Session4','Session5']:\n",
        "    session_path = os.path.join(folder, session)\n",
        "    # 'dialogue' folder contains Emotions and Transcripts\n",
        "    # 'sentences' folder contains Audios\n",
        "\n",
        "    trans_folder = os.path.join(session_path, 'dialog', 'transcriptions')\n",
        "\n",
        "    for trans_name in np.sort(os.listdir(trans_folder)):\n",
        "        if trans_name[:2] != '._':\n",
        "            emo_path = os.path.join(session_path, 'dialog', 'EmoEvaluation', trans_name)\n",
        "            with open(os.path.join(trans_folder, trans_name), encoding='utf8') as trans_file, open(emo_path, encoding='utf8') as emo_file:\n",
        "                conv_id += 1\n",
        "                turn_id = 0\n",
        "                for line in trans_file:\n",
        "                    audio_name, text = line.split(':')\n",
        "                    if trans_name.split('.')[0] in audio_name:\n",
        "                        turn_id += 1\n",
        "\n",
        "                        wav_path = os.path.join(session_path, 'sentences', 'wav', trans_name.split('.')[0], audio_name.split(' ')[0] + '.wav')\n",
        "\n",
        "                        reached = False\n",
        "                        count_em = {'Anger': 0, 'Happiness': 0, 'Sadness': 0, 'Neutral': 0, 'Frustration': 0, 'Excited': 0, 'Fear': 0, 'Surprise': 0, 'Disgust': 0, 'Other': 0}\n",
        "                        for emo_line in emo_file:\n",
        "                            if audio_name.split(' ')[0] in emo_line:\n",
        "                                emotion, vad = emo_line.split('\\t')[-2:]\n",
        "                                vad = vad[1:-2].split(',')\n",
        "                                reached = True\n",
        "                            elif emo_line[0] == 'C' and reached:\n",
        "                                evaluator = emo_line.split(':')[0]\n",
        "                                emotions = emo_line.split(':')[1].split('(')[0].split(';')\n",
        "                                emotions = [em.strip() for em in emotions]\n",
        "                                for em in emotions:\n",
        "                                    if em != '':\n",
        "                                        count_em[em] += 1\n",
        "                            elif reached:\n",
        "                                emo_file.seek(0)\n",
        "                                break\n",
        "\n",
        "\n",
        "                        row = {'session_id': int(session[-1]),\n",
        "                                'conv_id': conv_id,\n",
        "                                'turn_id': turn_id,\n",
        "                                'sentence': text.strip(),\n",
        "                                'path': wav_path,\n",
        "                                'emotion': emotion,\n",
        "                                'valence': float(vad[0]),\n",
        "                                'activation': float(vad[1]),\n",
        "                                'dominance': float(vad[2])\n",
        "                                }\n",
        "\n",
        "                        df.append(dict(**row, **count_em))\n",
        "\n",
        "df = pd.DataFrame(df)\n",
        "\n",
        "idx = np.array([os.path.exists(path) for path in df.path])\n",
        "print(f'Missing Audios: {np.sum(~idx)}')\n",
        "print('Missing Sentences:')\n",
        "print(df.iloc[~idx,3])\n",
        "df = df.iloc[idx, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7PJlyomnalv"
      },
      "source": [
        "## DATA INSPECTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiNWuXODzARC"
      },
      "source": [
        "### DATAFRAME INSPECTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inJ9gEVqedP4"
      },
      "outputs": [],
      "source": [
        "print(df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvlwuENdGiCi"
      },
      "outputs": [],
      "source": [
        "df_turn_count = df.groupby(['session_id','conv_id'])['conv_id'].count().rename('turns').reset_index()\n",
        "max_conv_length = df_turn_count['turns'].max()\n",
        "\n",
        "plt.figure(figsize=(20, 3))\n",
        "plt.title('Length of conversations', size=16)\n",
        "sns.countplot(x=df_turn_count['turns'])\n",
        "\n",
        "plt.ylabel('Count', size=12)\n",
        "plt.xlabel('Turns', size=12)\n",
        "plt.xticks(rotation = 90)\n",
        "sns.despine(top=True, right=True, left=False, bottom=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NU9TYMLFufcJ"
      },
      "outputs": [],
      "source": [
        "df_conv_count = df_turn_count.groupby(['session_id'])['session_id'].count().rename('convs').reset_index()\n",
        "df_conv_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxbDjugizKrO"
      },
      "source": [
        "### AUDIO INSPECTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axpKe4wS6U6j"
      },
      "outputs": [],
      "source": [
        "if use_audio_transformers:\n",
        "    quantile=0.1 # WE HANDLE ONLY 0.5% OF THE LENGTH OF THE AUDIOS RIGHT NOW\n",
        "    unpack_librosa_load = lambda *kwargs: kwargs[0]\n",
        "    max_seq_length = int(np.quantile([len(unpack_librosa_load(*librosa.load(p, sr=sample_rate))) for p in df.path], quantile))\n",
        "    max_seq_length = ((1 if (max_seq_length % sample_rate) != 0 else 0) + (max_seq_length//sample_rate)) * sample_rate\n",
        "    max_duration = max_seq_length//sample_rate\n",
        "    audio_transformer_params = {'sampling_rate':feature_extractor.sampling_rate, 'max_length':max_seq_length, 'truncation':True, 'padding':True}\n",
        "    print(f'Raw Audios truncated and padded to {max_duration} seconds') # quantile 1 -> 35 seconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySpzWXTdSs3a"
      },
      "outputs": [],
      "source": [
        "show_sample = 191\n",
        "Audio(df.iloc[show_sample, 4], autoplay=True) # ispection of the first audio (to change audio just change the first index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HA9NE7IsYTG"
      },
      "source": [
        "### TEXT INSPECTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bvRJgu3-ypp"
      },
      "outputs": [],
      "source": [
        "# LITE PROCESSING (THINK IF WE HAVE TO KEEP IT)\n",
        "def text_lite_preprocessing(data, column):\n",
        "  df = data.copy()\n",
        "  df[column] = df[column].apply(lambda x: str(x).lower())\n",
        "  df[column] = df[column].apply(lambda x: re.sub(r'\\d', \"number\", x))\n",
        "  df[column] = df[column].apply(lambda x: re.sub(\"_\", \"number\", x))\n",
        "  return (df)\n",
        "\n",
        "new_df = text_lite_preprocessing(df, 'sentence')\n",
        "\n",
        "show_sample = 1\n",
        "if use_text_transformers: # TRANSFORMER TOKENIZER\n",
        "    unpack_tokens = lambda **kwargs: (kwargs['input_ids'], kwargs['attention_mask'])\n",
        "    input_ids, attention_mask = unpack_tokens(**text_transformer_tokenizer(new_df.sentence.tolist(), **text_transformer_tokenizer_params))\n",
        "\n",
        "    # printing tokenization example\n",
        "    print(\"## Before tokenization: \")\n",
        "    print(new_df.loc[show_sample, 'sentence'])\n",
        "    print(\"\\n## After tokenization: \")\n",
        "    print(input_ids[show_sample, attention_mask[show_sample,:] == 1])\n",
        "    print(\"\\n## Token Meanings: \")\n",
        "    print(text_transformer_tokenizer.convert_ids_to_tokens(input_ids[show_sample,attention_mask[show_sample,:] == 1]))\n",
        "\n",
        "else: # OLD TOKENIZER\n",
        "    # creating tokenizer and fitting it on the training set\n",
        "    tokenizer = ks.preprocessing.text.Tokenizer(num_words = None, oov_token='<UNK>', split=' ', lower=False)\n",
        "    tokenizer.fit_on_texts(new_df.loc[new_df.session_id <= 3, 'sentence'])\n",
        "\n",
        "    # printing a tokenization example\n",
        "    print(\"## Before tokenization: \")\n",
        "    print(new_df.loc[show_sample,'sentence'])\n",
        "    print(\"\\n## After tokenization: \")\n",
        "    print(tokenizer.texts_to_sequences(new_df.sentence)[show_sample])\n",
        "    print('\\n## Token Meanings: ')\n",
        "    tokenizer_reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    print([tokenizer_reverse_word_map[s] for s in tokenizer.texts_to_sequences(new_df.sentence)[show_sample]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzTKFodxsXjP"
      },
      "outputs": [],
      "source": [
        "# drawing plot of the sentences length distribution\n",
        "if use_text_transformers:\n",
        "    lengths = np.sum(attention_mask, axis=1)\n",
        "else:\n",
        "    lengths = [len(sequence) for sequence in tokenizer.texts_to_sequences(new_df['sentence'])]\n",
        "\n",
        "quantile = 1 # was 0.99\n",
        "max_sentence_len = int(np.quantile(lengths, quantile))\n",
        "\n",
        "plt.hist(lengths, bins = len(set(lengths)))\n",
        "plt.title(f\"Sentence length distribution, thresh: (quantile ={quantile} -->{max_sentence_len} tokens) \")\n",
        "if quantile != 1:\n",
        "    plt.axvline(x = max_sentence_len, color = 'r', label = 'axvline - full height')\n",
        "plt.show()\n",
        "if use_text_transformers:\n",
        "    max_sentence_len = max_len_text_transformer\n",
        "    # emo_roberta_tokenizer_params['max_length'] = max_sentence_len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COsl8CRTy4Wl"
      },
      "source": [
        "### GOLDEN LABELS INSPECTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yE0Elc3taIO"
      },
      "source": [
        "Let's do some consideration of the following plotted distribution of the golden labels:\n",
        "\n",
        "- We can see how the emotion \"xxx\" is very common emotion (almost 2500 occurrences) but not very meaningful, our explanation behind the behavior of this golden label is that there isn't a predominant emotion in the classifications of the evaluators (excluding the self-evaluator) therefore an \"xxx\" gold label shows up.\n",
        "\n",
        "- We can see that a lot of our labels are undersampled in comparison to the other labels, data augmentation could help in this cases\n",
        "\n",
        "The strategies that we thinked of for handling the \"xxx\" labels are by considering them as missing values and trying to fill them by reconstruct a predominant emotion also considering the self-evaluators with more weights with respect to the other evaluators, but that would by definition modify the golden label we are provided with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDII47KtlAiy"
      },
      "outputs": [],
      "source": [
        "train_em  = df[df['session_id'] <= 3]['emotion']\n",
        "val_em    = df[df['session_id'] == 4]['emotion']\n",
        "test_em   = df[df['session_id'] == 5]['emotion']\n",
        "\n",
        "train_em_freq = {em: np.sum(train_em == em) / len(train_em) for em in label2id}\n",
        "val_em_freq   = {em: np.sum(val_em == em) / len(val_em)     for em in label2id}\n",
        "test_em_freq  = {em: np.sum(test_em == em) / len(test_em)   for em in label2id}\n",
        "\n",
        "n = len(label2id.keys())\n",
        "r = np.arange(n)\n",
        "width = 0.2\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(r, train_em_freq.values(), width = width, label='train')\n",
        "plt.bar(r + width, val_em_freq.values(), width = width, label='val')\n",
        "plt.bar(r + 2* width, test_em_freq.values(), width = width, label='test')\n",
        "plt.legend()\n",
        "plt.xticks(r + width, [em for em in label2id])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "locpR5kJzxl0"
      },
      "source": [
        "### VAD Inspection (Valence, Activation, Dominance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqMTz1M_wz4I"
      },
      "outputs": [],
      "source": [
        "interactive = False # CHANGE THIS TO False IN ORDER TO SEE AN INTERACTIVE PLOT\n",
        "title = \"VAD Inspection with reference to golden labels\"\n",
        "\n",
        "if interactive:\n",
        "    fig = px.scatter_3d(df, x='valence', y='activation', z='dominance',\n",
        "                        color='emotion',\n",
        "                        title=title)\n",
        "    fig.show()\n",
        "else:\n",
        "    sns.set(style = \"darkgrid\")\n",
        "\n",
        "    fig = plt.figure(figsize=(15,10), dpi= 100, facecolor='w', edgecolor='k')\n",
        "    ax = fig.add_subplot(111, projection = '3d')\n",
        "\n",
        "    for e in df.emotion.unique():\n",
        "        ax.scatter(df.valence[df.emotion == e], df.activation[df.emotion == e], df.dominance[df.emotion == e], label = e)\n",
        "\n",
        "    ax.set_xlabel(\"valence\")\n",
        "    ax.set_ylabel(\"activation\")\n",
        "    ax.set_zlabel(\"dominance\")\n",
        "\n",
        "    ax.legend()\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx5deRhm85dU"
      },
      "source": [
        "The only inspection for the VAD values that came in our mind is a visual inspection with reference to the golden labels and it seems that the golden labels are reflected in the VAD space with some clusters, that doesn't seem to be the case for the 'xxx' label that seems to be distributed quite randomly in this space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY2OF_wqhnWW"
      },
      "source": [
        "## Audio Features Extraction and Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAl78Ko5hYzV"
      },
      "source": [
        "The feature extracted from the audio are:\n",
        "- [zcr](https://librosa.org/doc/main/generated/librosa.feature.zero_crossing_rate.html): Zero Crossing Rate\n",
        "- [chroma_stft](https://librosa.org/doc/main/generated/librosa.feature.chroma_stft.html): Power Spectrogram\n",
        "- [mfcc](https://librosa.org/doc/main/generated/librosa.feature.mfcc.html): Mel-frequency cepstral coefficients\n",
        "- [rms](https://librosa.org/doc/main/generated/librosa.feature.rms.html): Root Mean Square value\n",
        "- [mel](https://librosa.org/doc/main/generated/librosa.feature.melspectrogram.html): mel-scaled spectrogram\n",
        "\n",
        "for each audio we create two new samples by slightly modifying the audio properties, the two augmentation technique adopted are:\n",
        "- noising\n",
        "- stretching & pitching\n",
        "\n",
        "the transcription and the labels related to this augmented audios are the same of the non augmented counterparts\n",
        "\n",
        "for each audio (augmented or not) we exctract the feaures described above and a new dataframe containing all this informations is created\n",
        "\n",
        "therefore at the moment we have:\n",
        "\n",
        "new_df.columns = \\[session_id, conv_id, turn_id, augmentation, sentence, zcr, chroma_stft_0-chroma_stft_11, mfcc_0-mfcc_19, rms, mel_0-mel_127, emotion, valence, activation, dominance, Anger, Happiness, Sadness, Neutral, Frustration, Excited, Fear, Surprise, Disgust, Other\\]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rpl1NdjirMpj"
      },
      "outputs": [],
      "source": [
        "def extract_features(df, feats, sr=sample_rate, save=False, feats_folder=None):\n",
        "    # calculate features and save them eventually\n",
        "    feats_dict = {feat: [] for feat in feats}\n",
        "\n",
        "    for path in df['path']:\n",
        "        audio, _ = librosa.load(path, sr=sr)\n",
        "\n",
        "        if 'mfcc' in feats:\n",
        "            # Mel-Frequency Cepstral Coefficients\n",
        "            feats_dict['mfcc'].append(librosa.feature.mfcc(y=audio, sr=sr).T)\n",
        "\n",
        "        if 'mel' in feats:\n",
        "            # Melspectrogram\n",
        "            feats_dict['mel'].append(librosa.feature.melspectrogram(y=audio, sr=sr).T)\n",
        "\n",
        "        if 'chroma' in feats:\n",
        "            # Chromagram\n",
        "            feats_dict['chroma'].append(librosa.feature.chroma_stft(y=audio, sr=sr).T)\n",
        "\n",
        "        if 'rms' in feats:\n",
        "            # Root-Mean-Square value\n",
        "            feats_dict['rms'].append(librosa.feature.rms(y=audio).T)\n",
        "\n",
        "        if 'zcr' in feats:\n",
        "            # Zero-Crossing Rate\n",
        "            feats_dict['zcr'].append(librosa.feature.zero_crossing_rate(y=audio).T)\n",
        "\n",
        "    if save:\n",
        "        for feat in feats:\n",
        "            with open(os.path.join(os.getcwd() if feats_folder is None else feats_folder, f'{feat}.pkl'), 'wb') as f:\n",
        "                pickle.dump(feats_dict[feat], f)\n",
        "\n",
        "    return feats_dict\n",
        "\n",
        "def load_features(df, feats, feats_folder=None):\n",
        "    feats_dict = {}\n",
        "    for feat in feats:\n",
        "        with open(os.path.join(os.getcwd() if feats_folder is None else feats_folder, f'{feat}.pkl'), \"rb\") as f:\n",
        "            feats_dict[feat] = pickle.load(f)\n",
        "    return feats_dict\n",
        "\n",
        "\n",
        "if not use_audio_transformers:\n",
        "    if use_drive:\n",
        "        new_drive_folder = 'NLP_Features'\n",
        "        features_folder = os.path.join(drive_folder, 'MyDrive', new_drive_folder)\n",
        "        if not os.path.exists(features_folder):\n",
        "            os.makedirs(features_folder)\n",
        "            print(f\"{new_drive_folder} folder is created in your drive!\\n\")\n",
        "    else:\n",
        "        features_folder = os.getcwd()\n",
        "\n",
        "    # feats_to_use = ['mfcc', 'mel', 'chroma', 'rms', 'zcr'] # shapes = [(t,20),(t,128),(t,12),(t,1),(t,1)]\n",
        "    feats_to_use = ['chroma']\n",
        "    override_features = False\n",
        "    if override_features:\n",
        "        audio_feats = extract_features(new_df, feats_to_use, save=True, feats_folder=features_folder)\n",
        "    else:\n",
        "        try:\n",
        "            audio_feats = load_features(new_df, feats_to_use, feats_folder=features_folder)\n",
        "        except:\n",
        "            audio_feats = extract_features(new_df, feats_to_use, save=True, feats_folder=features_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1M5steTOQfkZ"
      },
      "outputs": [],
      "source": [
        "standardize = lambda x: (x - x.mean(axis=0)) / x.std(axis=0) # over the time\n",
        "normalize = lambda x: (x - x.min(axis=0)) / (x.max(axis=0) - x.min(axis=0)) # over the time\n",
        "\n",
        "def normalize_and_reshape(array, perc):\n",
        "    #array = standardize(array)\n",
        "    array = normalize(array) # What about the minimum value that is mapped to 0, is then ignored??\n",
        "    if perc <= array.shape[0]:\n",
        "        return array[:perc]\n",
        "    else:\n",
        "        return np.concatenate([array, np.zeros((perc - array.shape[0], array.shape[1]))], axis=0)\n",
        "\n",
        "def process_features(feat_dict, perc=0.95):\n",
        "    feats = feat_dict.copy()\n",
        "    feat_list = feats.keys()\n",
        "    for feat in feat_list:\n",
        "        perc_val = int(np.quantile([x.shape[0] for x in feats[feat]], perc))\n",
        "        feats[feat] = [normalize_and_reshape(x, perc_val) for x in feats[feat]]\n",
        "    return feats\n",
        "\n",
        "if not use_audio_transformers:\n",
        "    audio_proc = process_features(audio_feats, perc=0.75)\n",
        "    show_sample = 191\n",
        "    plt.imshow(audio_proc[feats_to_use[0]][show_sample].T, cmap=cm.coolwarm)\n",
        "    plt.grid(False)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNaa3y5nQp32"
      },
      "outputs": [],
      "source": [
        "if use_audio_transformers:\n",
        "    columns = TAG_ID + ['sentence', 'path', 'emotion']\n",
        "    df_full = new_df.loc[:,columns]\n",
        "else:\n",
        "    columns = TAG_ID + ['sentence', 'emotion']\n",
        "    df_full = new_df.loc[:,columns]\n",
        "    for feat in feats_to_use:\n",
        "        df_full.loc[new_df.index.to_list(),feat] = pd.Series(audio_proc[feat], index=new_df.index)\n",
        "\n",
        "print(df_full.shape)\n",
        "df_full.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQfffRCaurGD"
      },
      "source": [
        "## Build data for our model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blXxW7ohu3YB"
      },
      "source": [
        "Here we can decide the data we want to exctract for training and evaluate our model, if we want we can exctract also the original data without augmentation and padding. for doing that just exctract the information you want from the dataframe and put them into numpy arrays.\n",
        "\n",
        "the data exctracted for the following model are:\n",
        "- augmented and padded data for training.\n",
        "- non augmented and padded data for validation and testing.\n",
        "\n",
        "The reason behind this choice is that training on augmented data could help the model to learn, but the validation and test data are not augmented because we want to evaluate our model on the original data, the fact that we pad the conversation shouldn't be a problem because we can specify to our metric to ignore the 'pad' labels therefore we can get consistent results.\n",
        "\n",
        "When we say that we use augmented data we mean that we use the data with augmentation 'none' to build a conversation, the data with augmentation 'noise' to build the same conversation but this conversation is all noisy, and we do the same for a new conversation all streched and pitched. in fact we can notice that the first dimension of the train_data is 270 that is 90 * 3 where 90 is the number of conversations we have in the original data and 3 is the number of augmentations techniques used.\n",
        "\n",
        "We can easily extend the conversations by combining the augmentation techniques in different ways, for example we create a conversation were the male actor is talking noisy and the female actor is talking normally, or we can noise a part of the audios from the male actor and the other audios of the male actor are normal (and here we are just talking on combinations of 'none' and 'noise' augmentations only but we have also 'stretch and pitch')\n",
        "\n",
        "also notice that we don't standardize our data before feeding it into the model and the reason behind this choice is due to the padding of the conversations.\n",
        "Given that we pad the conversations we have also to mask the added audios, and we mask them thanks to the masking layer that masks all the inputs that are 0.0.\n",
        "If we had standardized the data we would have lost the correspondence between the value 0 and the padded data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdQYY7OwxO3T"
      },
      "outputs": [],
      "source": [
        "def read_wav_file(file_path, sr=sample_rate):\n",
        "    if file_path == '':\n",
        "        audio = np.array([])\n",
        "    else:\n",
        "        audio, sample_rate = librosa.load(file_path, sr=sr)\n",
        "        if sample_rate != sr:\n",
        "            raise ValueError(f\"sample rate (={sample_rate}) of your files must be {sr}\")\n",
        "    return audio\n",
        "\n",
        "def get_text_data(df, tokenizer, text_tokenizer_params, conv_length, use_text_transformers):\n",
        "    conv_ids = []\n",
        "    for conv_id in df.conv_id.unique():\n",
        "        padded_texts = ['']*(conv_length-1) + df[df.conv_id == conv_id].sentence.to_list()\n",
        "        convs = [padded_texts[i:i+conv_length] for i in range(len(padded_texts)-conv_length+1)]\n",
        "        if use_text_transformers:\n",
        "            conv_ids += [unpack_tokens(**tokenizer(conv, **text_tokenizer_params)) for conv in convs]\n",
        "        else:\n",
        "            conv_ids += [np.array(ks.utils.pad_sequences(tokenizer.texts_to_sequences(conv), **text_tokenizer_params)) for conv in convs]\n",
        "    if use_text_transformers:\n",
        "        return np.array([conv[0] for conv in conv_ids]) , np.array([conv[1] for conv in conv_ids])\n",
        "    return np.array(conv_ids)\n",
        "\n",
        "def get_audio_data(df, feats_to_use, conv_length, use_audio_transformers, audio_params=None):\n",
        "    if use_audio_transformers:\n",
        "        unpack_audios = lambda **kwargs: (np.vstack(kwargs['input_values']), np.vstack(kwargs['attention_mask']))\n",
        "    convs = []\n",
        "    for conv_id in df.conv_id.unique():\n",
        "        if use_audio_transformers:\n",
        "            if audio_params is None:\n",
        "                raise Exception(\"You need to set audio_params\")\n",
        "            padded_conv = [''] * (conv_length -1) + df[df.conv_id == conv_id][feats_to_use].to_list()\n",
        "            for i in range(len(padded_conv) - conv_length + 1):\n",
        "                convs += padded_conv[i:i+conv_length]\n",
        "        else:\n",
        "            audios = [np.concatenate([row[feat] for feat in feats_to_use], axis=1) for _, row in df[df.conv_id == conv_id].iterrows()]\n",
        "            padded_audios = [np.zeros(audios[0].shape)] * (conv_length - 1) + audios\n",
        "            convs += [padded_audios[i:i+conv_length] for i in range(len(padded_audios)-conv_length+1)]\n",
        "    if use_audio_transformers:\n",
        "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "        convs, convs_id = unpack_audios(**feature_extractor(pd.Series(convs).map(read_wav_file).to_list(), **audio_transformer_params))\n",
        "        warnings.filterwarnings(\"default\", category=RuntimeWarning)\n",
        "        return convs.reshape((df.shape[0], conv_length, -1)) , convs_id.reshape((df.shape[0], conv_length, -1))\n",
        "    return np.array(convs)\n",
        "conv_length = 3\n",
        "use_audio = True\n",
        "use_text = False\n",
        "\n",
        "\n",
        "# DATA SPLITTING\n",
        "df_train  = df_full[df_full['session_id'] <= 3]\n",
        "df_val    = df_full[df_full['session_id'] == 4]\n",
        "df_test   = df_full[df_full['session_id'] == 5]\n",
        "\n",
        "\n",
        "# PREPARE AUDIOS\n",
        "if use_audio:\n",
        "    if use_audio_transformers:\n",
        "        audio_train, audio_train_mask = get_audio_data(df_train, 'path', conv_length, use_audio_transformers, audio_params=audio_transformer_params)\n",
        "        audio_val, audio_val_mask = get_audio_data(df_val, 'path', conv_length, use_audio_transformers, audio_params=audio_transformer_params)\n",
        "        audio_test, audio_test_mask = get_audio_data(df_test, 'path', conv_length, use_audio_transformers, audio_params=audio_transformer_params)\n",
        "    else:\n",
        "        audio_train = get_audio_data(df_train, feats_to_use, conv_length, use_audio_transformers)\n",
        "        audio_val = get_audio_data(df_val, feats_to_use, conv_length, use_audio_transformers)\n",
        "        audio_test = get_audio_data(df_test, feats_to_use, conv_length, use_audio_transformers)\n",
        "\n",
        "# PREPARE TEXTS\n",
        "if use_text:\n",
        "    if use_text_transformers:\n",
        "        text_data_params = (text_transformer_tokenizer, text_transformer_tokenizer_params, conv_length, use_text_transformers)\n",
        "        text_train_ids, text_train_mask = get_text_data(df_train, *text_data_params)\n",
        "        text_val_ids, text_val_mask = get_text_data(df_val, *text_data_params)\n",
        "        text_test_ids, text_test_mask = get_text_data(df_test, *text_data_params)\n",
        "    else:\n",
        "        text_data_params = (tokenizer, {'truncating': 'post', 'padding': 'post', 'maxlen': max_sentence_len}, conv_length, use_text_transformers)\n",
        "        text_train_ids = get_text_data(df_train, *text_data_params)\n",
        "        text_val_ids = get_text_data(df_val, *text_data_params)\n",
        "        text_test_ids = get_text_data(df_test, *text_data_params)\n",
        "\n",
        "# PREPARE LABELS\n",
        "em_train = tf.convert_to_tensor(ks.utils.to_categorical(df_train['emotion'].map(label2id),  num_classes=11))\n",
        "em_val = tf.convert_to_tensor(ks.utils.to_categorical(df_val['emotion'].map(label2id),  num_classes=11))\n",
        "em_test = tf.convert_to_tensor(ks.utils.to_categorical(df_test['emotion'].map(label2id),  num_classes=11))\n",
        "\n",
        "# FULL DATA\n",
        "train_data = []\n",
        "val_data = []\n",
        "test_data = []\n",
        "\n",
        "if use_audio:\n",
        "    train_data.append(audio_train)\n",
        "    val_data.append(audio_val)\n",
        "    test_data.append(audio_test)\n",
        "\n",
        "if use_text:\n",
        "    train_data.append(text_train_ids)\n",
        "    val_data.append(text_val_ids)\n",
        "    test_data.append(text_test_ids)\n",
        "\n",
        "if use_audio_transformers and use_audio:\n",
        "    train_data.append(audio_train_mask)\n",
        "    val_data.append(audio_val_mask)\n",
        "    test_data.append(audio_test_mask)\n",
        "\n",
        "if use_text_transformers and use_text:\n",
        "    train_data.append(text_train_mask)\n",
        "    val_data.append(text_val_mask)\n",
        "    test_data.append(text_test_mask)\n",
        "\n",
        "# CHECK SHAPES\n",
        "\n",
        "if use_audio:\n",
        "    audio_feats_shape = audio_train.shape[2:]\n",
        "\n",
        "print('Shapes:')\n",
        "if use_text and use_audio:\n",
        "    print(f'  -audio_train : {audio_train.shape}\\t-text_train : {text_train_ids.shape}\\t-em_train : {em_train.shape}')\n",
        "    print(f'  -audio_val   : {audio_val.shape}\\t-text_val   : {text_val_ids.shape}\\t-em_val   : {em_val.shape}')\n",
        "    print(f'  -audio_test  : {audio_test.shape}\\t-text_test  : {text_test_ids.shape}\\t-em_test  : {em_test.shape}')\n",
        "else:\n",
        "    if use_audio:\n",
        "        print(f'  -audio_train : {audio_train.shape}\\t-em_train : {em_train.shape}')\n",
        "        print(f'  -audio_val   : {audio_val.shape}\\t-em_val   : {em_val.shape}')\n",
        "        print(f'  -audio_test  : {audio_test.shape}\\t-em_test  : {em_test.shape}')\n",
        "    if use_text:\n",
        "        print(f'  -text_train : {text_train_ids.shape}\\t-em_train : {em_train.shape}')\n",
        "        print(f'  -text_val   : {text_val_ids.shape}\\t-em_val   : {em_val.shape}')\n",
        "        print(f'  -text_test  : {text_test_ids.shape}\\t-em_test  : {em_test.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWnm6Xd919Rn"
      },
      "source": [
        "## Model Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEyt5Mhj2BEO"
      },
      "source": [
        "The model is thinked to take into account conversations in the batch dimension and not the turns.\n",
        "\n",
        "Below is showed a visual representation of the CNN_on_Feature_Cell class that is the big box, the inputs for that class are given by the RNN wrapper class of tensorflow that can take as input also Custom Cells\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1aBHLPX74TcdVJP1zYMYp_1t8o5dpfbPr'>\n",
        "\n",
        "the audio embedding take inputs of dimensions (162,1) because 162 are the features exctracted from the audios and the last dimension is used to apply Conv1D of keras (that are present inside the audio embedding).\n",
        "\n",
        "The audio embedding output can be a vector of arbitrary dimension that is concatenated to the state (who's dimension can be specified as an hyperparameter).\n",
        "\n",
        "The concatenation is then passed as an input to two dense layers, one that specifies the new state and one that specifies the output (who's dimension can be specified as an hyperparameter)\n",
        "\n",
        "now i will describe the full model:\n",
        "\n",
        "- what we expect in input to our model is the conversations padded, each audio in the conversation have the 162 features pre-processed (X_1 are the features extracted from the first audio of the conversation)\n",
        "\n",
        "- first we mask the input to tell the model what to ignore\n",
        "\n",
        "- then we reshape the input to make it suitable for the convolutions\n",
        "\n",
        "- then we pass the input masked and reshaped to the designed cell thanks to the RNN wrapper. Each output of the cell represent an embedding that take in consideration both X_t and S_t-1 (in practice Y_1 for example takes into account X_1 and S_0 that is full of zeros, then Y_2 takes into account X_2 and S_1 that mantains memory of the past audios)\n",
        "\n",
        "- then we take all the Y produced and we apply a dense layer which produce an output of 32 units followed by a dropout for all the Y (that means for each turn of the conversation)\n",
        "\n",
        "- then for each turn of the conversation we have an output of 12 because we have 11 emotions and the pad class\n",
        "\n",
        "\n",
        "The problem of this model is that most likely he suffers the vanishing gradient problem (look at the history plot below), the next step will be trying to use a GRU or LSTM as a wrapper for the cell, this has to be investigated.\n",
        "\n",
        "The results are saved on my drive and can be loaded at any time given that i setted the variables use_drive=True and override_cnn_on_features=True\n",
        "\n",
        "NB: BELOW YOU WILL FIND COMMENTS I'M STILL WORKING ON IT\n",
        "\n",
        "UPDATE!\n",
        "- I moved the audio embedder outside of the cell and used a time distributed wrapper on it, then i substituted the remaining part of the cell in the upper figure with a GRU layer, that should suffice in keeping track of the past history of the embedded audios, in this way the model description should be more clear also at code level.\n",
        "\n",
        "- The model wasn't taking into account the masked input in the computation of the loss. the mask was used only in order to speed up the computation of the model. therefore i keeped the mask and also added the sample_weights parameters, the results are more likely to be correct, the high score of the previous version were due to the fact that i was considering also the 'pad' labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPsWJENE167H"
      },
      "outputs": [],
      "source": [
        "def set_seeds(seed):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "    tf.random.set_seed(random_seed)\n",
        "\n",
        "def set_global_determinism(seed):\n",
        "    set_seeds(seed=seed)\n",
        "\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "\n",
        "    try:\n",
        "        tf.config.threading.set_inter_op_parallelism_threads(1)\n",
        "        tf.config.threading.set_intra_op_parallelism_threads(1)\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", str(e))\n",
        "\n",
        "# Call the above function with seed value\n",
        "set_global_determinism(seed=random_seed)\n",
        "classic_behavior = False\n",
        "if classic_behavior:\n",
        "    glorot_initializer = \"glorot_uniform\"\n",
        "    uniform_initializer = \"uniform\"\n",
        "    orthogonal_initializer = \"orthogonal\"\n",
        "else:\n",
        "    glorot_initializer = ks.initializers.glorot_uniform(seed=random_seed)\n",
        "    uniform_initializer = ks.initializers.RandomUniform(seed=random_seed)\n",
        "    orthogonal_initializer = ks.initializers.Orthogonal(seed=random_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epO6fwZlVV-y"
      },
      "outputs": [],
      "source": [
        "latent_embedding_dim = 256\n",
        "\n",
        "def create_audio_transformer_model(audio_transformer, max_seq_len, latent_embedding_dim, trainable=False):\n",
        "    audio_transformer.layers[0].trainable = trainable\n",
        "\n",
        "    input_values = ks.Input(shape=(max_seq_len,))\n",
        "    attention_mask = ks.Input(shape=(max_seq_len,))\n",
        "\n",
        "    output = audio_transformer(input_values=input_values, attention_mask=attention_mask)\n",
        "\n",
        "    output = ks.layers.GlobalAveragePooling1D()(output.last_hidden_state)\n",
        "\n",
        "    output = ks.layers.Dense(units=latent_embedding_dim, activation='relu', kernel_initializer=glorot_initializer)(output)\n",
        "\n",
        "    model = ks.models.Model(inputs = [input_values,attention_mask] ,outputs = output)\n",
        "    return model\n",
        "\n",
        "if use_audio:\n",
        "    if use_audio_transformers:\n",
        "        audio_embedder = create_audio_transformer_model(wav2vec2, max_seq_length, latent_embedding_dim)\n",
        "    else:\n",
        "        audio_embedder = ks.models.Sequential([\n",
        "                ks.Input(audio_feats_shape),\n",
        "\n",
        "                ks.layers.Conv1D(2*audio_feats_shape[-1], 3, strides=1, padding='same', activation='relu', kernel_initializer=glorot_initializer),\n",
        "                ks.layers.Conv1D(2*audio_feats_shape[-1], 3, strides=2, padding='same', activation='relu', kernel_initializer=glorot_initializer),\n",
        "\n",
        "                ks.layers.Conv1D(4*audio_feats_shape[-1], 3, strides=1, padding='same', activation='relu', kernel_initializer=glorot_initializer),\n",
        "                ks.layers.Conv1D(4*audio_feats_shape[-1], 3, strides=2, padding='same', activation='relu', kernel_initializer=glorot_initializer),\n",
        "\n",
        "                ks.layers.Conv1D(8*audio_feats_shape[-1], 3, strides=1, padding='same', activation='relu', kernel_initializer=glorot_initializer),\n",
        "                ks.layers.Conv1D(8*audio_feats_shape[-1], 3, strides=2, padding='same', activation='relu', kernel_initializer=glorot_initializer),\n",
        "\n",
        "                ks.layers.Bidirectional(ks.layers.LSTM(8*audio_feats_shape[-1], return_sequences=True, kernel_initializer=glorot_initializer, recurrent_initializer=orthogonal_initializer)),\n",
        "                ks.layers.Bidirectional(ks.layers.LSTM(8*audio_feats_shape[-1], kernel_initializer=glorot_initializer, recurrent_initializer=orthogonal_initializer)),\n",
        "\n",
        "                #ks.layers.Conv1D(16*audio_feats_shape[-1], 3, strides=1, padding='same', activation='relu', kernel_initializer=glorot_initializer),\n",
        "                #ks.layers.Conv1D(16*audio_feats_shape[-1], 3, strides=2, padding='same', activation='relu', kernel_initializer=glorot_initializer),\n",
        "\n",
        "                #ks.layers.Conv1D(32*audio_feats_shape[-1], 3, strides=1, padding='same', activation='relu', kernel_initializer=glorot_initializer),\n",
        "                #ks.layers.Conv1D(32*audio_feats_shape[-1], 3, strides=2, padding='same', activation='relu', kernel_initializer=glorot_initializer),\n",
        "\n",
        "                #ks.layers.GlobalAveragePooling1D(), #ks.layers.GlobalMaxPooling1D()\n",
        "                ks.layers.Dense(latent_embedding_dim, kernel_initializer=glorot_initializer)\n",
        "    ])\n",
        "    audio_embedder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0XI-cmP74te"
      },
      "outputs": [],
      "source": [
        "def create_text_transformer_model(text_transformer, max_len, latent_embedding_dim, trainable=False):\n",
        "    text_transformer.layers[0].trainable = trainable\n",
        "\n",
        "    input_ids = ks.Input(shape=(max_len,),dtype='int32')\n",
        "    attention_mask = ks.Input(shape=(max_len,),dtype='int32')\n",
        "\n",
        "    output = text_transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    # maybe we can use batch normalization\n",
        "\n",
        "    output = ks.layers.GlobalAveragePooling1D()(output.last_hidden_state)\n",
        "\n",
        "    output = ks.layers.Dense(units=latent_embedding_dim, activation='relu', kernel_initializer=glorot_initializer)(output)\n",
        "\n",
        "    model = ks.models.Model(inputs = [input_ids,attention_mask] ,outputs = output)\n",
        "\n",
        "    return model\n",
        "\n",
        "if use_text:\n",
        "    if use_text_transformers:\n",
        "        text_embedder = create_text_transformer_model(text_transformer, max_len_text_transformer, latent_embedding_dim)\n",
        "    else:\n",
        "        text_embedder = ks.models.Sequential([\n",
        "                ks.layers.Embedding(tokenizer.document_count, 256, input_length=max_sentence_len, mask_zero=True, embeddings_initializer=uniform_initializer),\n",
        "                ks.layers.Bidirectional(ks.layers.LSTM(256, return_sequences=True, kernel_initializer=glorot_initializer, recurrent_initializer=orthogonal_initializer)),\n",
        "                ks.layers.Bidirectional(ks.layers.LSTM(256, kernel_initializer=glorot_initializer, recurrent_initializer=orthogonal_initializer)),\n",
        "                ks.layers.Dense(units=latent_embedding_dim, activation='relu', kernel_initializer=glorot_initializer)\n",
        "        ])\n",
        "\n",
        "    text_embedder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uf7SRtrxejmi"
      },
      "outputs": [],
      "source": [
        "def make_multimodal_model(audio_embedder, audio_feats_shape, text_embedder, max_sentence_len,\n",
        "                          deep_embedding_dim, conversation_dim, num_classes,\n",
        "                          use_audio_attention, use_text_attention,\n",
        "                          fusion_mode='concatenate', mask_conv = False,\n",
        "                          use_audio=True, use_text=True):\n",
        "    if not (use_audio or use_text):\n",
        "        raise Exception(\"You need to set at True at least one of use_audio or use_text\")\n",
        "\n",
        "    if fusion_mode=='concatenate' and (use_audio and use_text):\n",
        "        deep_embedding_dim = 2 * deep_embedding_dim\n",
        "\n",
        "    if use_audio:\n",
        "        audio_input_shape = (conversation_dim, *audio_feats_shape)\n",
        "    if use_text:\n",
        "        text_input_shape = (conversation_dim, max_sentence_len)\n",
        "\n",
        "    # INPUTS\n",
        "    inputs = []\n",
        "    if use_audio:\n",
        "        audio_input = ks.Input(audio_input_shape, name='audio_input')\n",
        "        inputs.append(audio_input)\n",
        "    if use_text:\n",
        "        text_input = ks.Input(text_input_shape, name='text_input')\n",
        "        inputs.append(text_input)\n",
        "    if use_audio_attention and use_audio:\n",
        "        audio_attention_input = ks.Input(audio_input_shape, name='audio_attention_mask')\n",
        "        inputs.append(audio_attention_input)\n",
        "    if use_text_attention and use_text:\n",
        "        text_attention_input = ks.Input(text_input_shape, name='text_attention_mask')\n",
        "        inputs.append(text_attention_input)\n",
        "\n",
        "    # AUDIO EMBEDDER\n",
        "    if use_audio:\n",
        "        if use_audio_attention:\n",
        "            if mask_conv:\n",
        "                x_audio = ks.layers.TimeDistributed(audio_embedder)([audio_input, audio_attention_input], mask=ks.backend.all(audio_attention_input[...,2:]==0, axis=-1))\n",
        "            else:\n",
        "                x_audio = ks.layers.TimeDistributed(audio_embedder)([audio_input, audio_attention_input])\n",
        "        else:\n",
        "            if mask_conv:\n",
        "                x_audio = ks.layers.TimeDistributed(audio_embedder)(audio_input, mask=ks.backend.all(audio_input==0, axis=-1))\n",
        "            else:\n",
        "                x_audio = ks.layers.TimeDistributed(audio_embedder)(audio_input)\n",
        "\n",
        "    # TEXT EMBEDDER\n",
        "    if use_text:\n",
        "        if use_text_attention:\n",
        "            if mask_conv:\n",
        "                x_text = ks.layers.TimeDistributed(text_embedder)([text_input, text_attention_input], mask=ks.backend.all(text_attention_input[...,2:]==0, axis=-1))\n",
        "            else:\n",
        "                x_text = ks.layers.TimeDistributed(text_embedder)([text_input, text_attention_input])\n",
        "        else:\n",
        "            if mask_conv:\n",
        "                x_text = ks.layers.TimeDistributed(text_embedder)(text_input, mask=ks.backend.all(text_input==0, axis=-1))\n",
        "            else:\n",
        "                x_text = ks.layers.TimeDistributed(text_embedder)(text_input)\n",
        "\n",
        "    # AUDIO AND TEXT FUSION\n",
        "    if use_audio and use_text:\n",
        "        if fusion_mode == 'concatenate':\n",
        "            skip_x = ks.layers.concatenate([x_audio, x_text], axis=-1)\n",
        "        if fusion_mode == 'add':\n",
        "            skip_x = ks.layers.Add()([x_audio, x_text])\n",
        "    else:\n",
        "        if use_audio:\n",
        "            skip_x = x_audio\n",
        "        if use_text:\n",
        "            skip_x = x_text\n",
        "\n",
        "    x = ks.layers.LSTM(deep_embedding_dim, kernel_initializer=glorot_initializer, recurrent_initializer=orthogonal_initializer)(skip_x)\n",
        "\n",
        "    x = ks.layers.Add()([x, skip_x[:,-1,:]])\n",
        "\n",
        "    x = ks.layers.Dense(units=deep_embedding_dim/2, activation='relu', kernel_initializer=glorot_initializer)(x)\n",
        "\n",
        "    output = ks.layers.Dense(units=num_classes, activation='softmax', kernel_initializer=glorot_initializer)(x)\n",
        "\n",
        "    model = ks.models.Model(inputs, output)\n",
        "\n",
        "    return model\n",
        "\n",
        "fusion_mode = 'concatenate' # concatenate, add\n",
        "make_multimodal_params = {\n",
        "    'audio_embedder': audio_embedder if use_audio else None,\n",
        "    'audio_feats_shape': audio_feats_shape if use_audio else None,\n",
        "    'text_embedder': text_embedder if use_text else None,\n",
        "    'max_sentence_len': max_sentence_len if use_text else None,\n",
        "    'deep_embedding_dim': latent_embedding_dim,\n",
        "    'conversation_dim': conv_length,\n",
        "    'num_classes': len(label2id),\n",
        "    'use_audio_attention': use_audio_transformers,\n",
        "    'use_text_attention': use_text_transformers,\n",
        "    'fusion_mode': fusion_mode,\n",
        "    'use_audio': use_audio,\n",
        "    'use_text': use_text\n",
        "}\n",
        "model = make_multimodal_model(**make_multimodal_params)\n",
        "ks.utils.plot_model(model, show_shapes=True, dpi=60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTSDM5iCBzJ-"
      },
      "outputs": [],
      "source": [
        "class Masked_Categorical_Accuracy(ks.metrics.Metric):\n",
        "    def __init__(self, ignore_indexes = [], name='categorical_accuracy', **kwargs):\n",
        "        super(Masked_Categorical_Accuracy, self).__init__(name=name, **kwargs)\n",
        "        self.ignore_indexes = ignore_indexes\n",
        "        self.matches = self.add_weight(name='matches', initializer='zeros')\n",
        "        self.total = self.add_weight(name='total', initializer='zeros')\n",
        "\n",
        "    def update_state(self, label, pred, sample_weight=None):\n",
        "        pred = tf.argmax(pred, axis=-1)\n",
        "        label = tf.cast(tf.argmax(label, axis=-1), pred.dtype)\n",
        "        matches = label == pred\n",
        "\n",
        "        mask = tf.cast(tf.ones_like(matches), dtype='bool')\n",
        "        for idx in self.ignore_indexes:\n",
        "            mask = mask & (label != idx)\n",
        "\n",
        "        matches = matches & mask\n",
        "        matches = tf.cast(matches, dtype=tf.float32)\n",
        "        mask = tf.cast(mask, dtype=tf.float32)\n",
        "        self.matches.assign_add(tf.reduce_sum(matches))\n",
        "        self.total.assign_add(tf.reduce_sum(mask))\n",
        "\n",
        "    def result(self):\n",
        "        return self.matches.value()/self.total.value()\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.matches.assign(0.0)\n",
        "        self.total.assign(0.0)\n",
        "\n",
        "\n",
        "class Masked_F1(ks.metrics.Metric):\n",
        "    def __init__(self, num_labels, mode='macro', name=None, ignore_indexes = [], dataset_distribution=None, **kwargs):\n",
        "        # mode='weighted' and dataset_distribution=None is equivalent to mode='macro'\n",
        "        super(Masked_F1, self).__init__(name=name, **kwargs)\n",
        "\n",
        "        self.num_labels = num_labels\n",
        "        self.mode = mode\n",
        "        self.ignore_indexes = ignore_indexes\n",
        "        self.dataset_distribution = dataset_distribution\n",
        "\n",
        "        self.tp = tf.Variable(np.zeros((num_labels,)), shape=(num_labels,))\n",
        "        self.fp = tf.Variable(np.zeros((num_labels,)), shape=(num_labels,))\n",
        "        self.fn = tf.Variable(np.zeros((num_labels,)), shape=(num_labels,))\n",
        "\n",
        "    def set_split_params(self, dataset_distribution):\n",
        "        # the distribution changes continuously when fit is called (is setted to train and val thanks to ChangeTrainValBehaviour custom Callback)\n",
        "        self.dataset_distribution = dataset_distribution\n",
        "\n",
        "    def update_state(self, label, pred, sample_weight=None):\n",
        "        batch_size = tf.size(label) / np.prod(label.shape[1:])\n",
        "        shape = (batch_size, *label.shape[1:])\n",
        "\n",
        "        if len(label.shape) >= 3:\n",
        "            label = ks.backend.reshape(label, (batch_size * np.prod(label.shape[1:-1]),label.shape[-1]))\n",
        "            pred = ks.backend.reshape(pred, (batch_size * np.prod(pred.shape[1:-1]),pred.shape[-1]))\n",
        "\n",
        "        label = ks.backend.cast(ks.backend.equal(label, ks.backend.max(label, axis=1, keepdims=True)), tf.float32)\n",
        "        pred = ks.backend.cast(ks.backend.equal(pred, ks.backend.max(pred, axis=1, keepdims=True)), tf.float32)\n",
        "\n",
        "        tp = ks.backend.sum(ks.backend.cast(label*pred, 'float'), axis=0)\n",
        "        fp = ks.backend.sum(ks.backend.cast((1-label)*pred, 'float'), axis=0)\n",
        "        fn = ks.backend.sum(ks.backend.cast(label*(1-pred), 'float'), axis=0)\n",
        "\n",
        "        self.tp.assign_add(tf.cast(tp, self.tp.value().dtype))\n",
        "        self.fp.assign_add(tf.cast(fp, self.fp.value().dtype))\n",
        "        self.fn.assign_add(tf.cast(fn, self.fn.value().dtype))\n",
        "\n",
        "    def result(self):\n",
        "        tp = self.tp.value()\n",
        "        fn = self.fn.value()\n",
        "        fp = self.fp.value()\n",
        "\n",
        "        f1 = 2*tp / (2*tp+fn+fp+ks.backend.epsilon())\n",
        "\n",
        "        mask = tf.cast(tf.ones_like(f1), dtype='bool')\n",
        "        for idx in self.ignore_indexes:\n",
        "            mask = mask & (tf.convert_to_tensor(np.arange(self.num_labels)) != idx)\n",
        "        mask = tf.cast(mask, dtype=f1.dtype)\n",
        "\n",
        "        f1 *= mask\n",
        "\n",
        "        if self.mode == 'none':\n",
        "            return f1\n",
        "        if self.mode == 'macro':\n",
        "            return ks.backend.sum(f1)/ks.backend.sum(mask)\n",
        "        if self.mode == 'weighted':\n",
        "            if self.dataset_distribution is None:\n",
        "                dataset_distribution = tf.ones_like(f1) / ks.backend.sum(mask)\n",
        "            else:\n",
        "                dataset_distribution = tf.convert_to_tensor(self.dataset_distribution, dtype=f1.dtype) * mask\n",
        "                dataset_distribution /= ks.backend.sum(dataset_distribution)\n",
        "            return ks.backend.sum(f1 * dataset_distribution)\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.tp.assign(tf.zeros_like(self.tp.value()))\n",
        "        self.fp.assign(tf.zeros_like(self.fp.value()))\n",
        "        self.fn.assign(tf.zeros_like(self.fn.value()))\n",
        "\n",
        "\n",
        "class ChangeTrainValBehaviour(ks.callbacks.Callback):\n",
        "    def __init__(self, monitor_metrics=[], train_params=[], val_params=[]):\n",
        "        self.monitor_metrics = monitor_metrics\n",
        "        self.train_params = train_params\n",
        "        self.val_params = val_params\n",
        "\n",
        "    def on_train_begin(self, logs):\n",
        "        for i in range(len(self.monitor_metrics)):\n",
        "            self.monitor_metrics[i].set_split_params(**self.train_params[i])\n",
        "\n",
        "    def on_test_begin(self, logs):\n",
        "        for i in range(len(self.monitor_metrics)):\n",
        "            self.monitor_metrics[i].set_split_params(**self.val_params[i])\n",
        "\n",
        "    def on_test_end(self,  logs):\n",
        "        for i in range(len(self.monitor_metrics)):\n",
        "            self.monitor_metrics[i].set_split_params(**self.train_params[i])\n",
        "\n",
        "\n",
        "# PREPARE SAMPLE WEIGHTS\n",
        "ignore_emotions = [em for em in label2id.keys() if em not in EMO_SIX]\n",
        "bin_counts = np.bincount(df_train['emotion'].map(label2id))\n",
        "class_weights = np.sum(bin_counts[[label2id[c] for c in EMO_SIX]])/(len(EMO_SIX)*bin_counts)\n",
        "class_weights[[label2id[c] for c in ignore_emotions]] = 0\n",
        "class_weights = {i:class_weights[i] for i in range(class_weights.size)}\n",
        "\n",
        "# calculating the counting for each emotion using one of the sets (or combination of them) and ordering them in the same order of the network labels\n",
        "train_freq  = df_train['emotion'].value_counts()\n",
        "train_distr = {em: train_freq[em] / np.sum(train_freq[EMO_SIX]) if em in EMO_SIX else 0.0 for em in label2id.keys()}\n",
        "\n",
        "val_freq    = df_val['emotion'].value_counts()\n",
        "val_distr   = {em: val_freq[em] / np.sum(val_freq[EMO_SIX]) if em in EMO_SIX else 0.0 for em in label2id.keys()}\n",
        "\n",
        "test_freq   = df_test['emotion'].value_counts()\n",
        "test_distr  = {em: test_freq[em] / np.sum(test_freq[EMO_SIX]) if em in EMO_SIX else 0.0 for em in label2id.keys()}\n",
        "\n",
        "f1_weighted_train_params = {'dataset_distribution': list(train_distr.values())}\n",
        "f1_weighted_val_params = {'dataset_distribution': list(val_distr.values())}\n",
        "\n",
        "print(train_distr)\n",
        "print(class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDIIqB4OrmfI"
      },
      "outputs": [],
      "source": [
        "use_class_weights = False\n",
        "\n",
        "model_name = f'conv{conv_length}_'\n",
        "model_name += f'{fusion_mode}_' if use_audio and use_text else \"\"\n",
        "if use_audio:\n",
        "    model_name += \"AudioTransformer\" if use_audio_transformers else f'SimpleAudioModel_{\"_\".join([f for f in feats_to_use])}'\n",
        "if use_audio and use_text:\n",
        "    model_name += \"&\"\n",
        "if use_text:\n",
        "    model_name += \"TextTransformer\" if use_text_transformers else \"SimpleTextModel\"\n",
        "model_name += f'_{\"cw_\" if use_class_weights else \"\"}'\n",
        "model_name += f's{random_seed}'\n",
        "model_path = os.path.join(models_folder, f'{model_name}.h5')\n",
        "print(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "u4opJCMKNky6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXbxTOdk5n0B"
      },
      "outputs": [],
      "source": [
        "if not use_class_weights:\n",
        "    for emo in EMO_SIX:\n",
        "        class_weights[emo] = 1.0\n",
        "    train_sample_weights = np.array(df_train['emotion'].map(label2id).map(class_weights).to_list())\n",
        "    val_sample_weights = np.array(df_val['emotion'].map(label2id).map(class_weights).to_list())\n",
        "    test_sample_weights = np.array(df_test['emotion'].map(label2id).map(class_weights).to_list())\n",
        "\n",
        "override_model = True\n",
        "batch_size = 16\n",
        "if override_model:\n",
        "    # COMPILE\n",
        "    optimizer = ks.optimizers.Adam(1e-3)\n",
        "\n",
        "    loss = ks.losses.CategoricalCrossentropy(name='loss')\n",
        "    categorical_accuracy = Masked_Categorical_Accuracy(ignore_indexes = [label2id[emo] for emo in ignore_emotions], name='categorical_accuracy')\n",
        "    f1_weighted = Masked_F1(len(label2id), mode='weighted', name='f1_weighted', ignore_indexes=[label2id[emo] for emo in ignore_emotions], dataset_distribution=list(train_distr.values()))\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=[categorical_accuracy, f1_weighted], weighted_metrics=[])\n",
        "\n",
        "    # TRAIN\n",
        "    callbacks = [\n",
        "            ks.callbacks.ModelCheckpoint(model_path, save_weights_only=True, save_best_only = True, monitor=\"val_f1_weighted\", mode='max', verbose=1),\n",
        "            ks.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=2, min_lr=1e-6),\n",
        "            ks.callbacks.EarlyStopping(monitor='val_loss', patience=6, verbose=1, mode='min'),\n",
        "            ChangeTrainValBehaviour(monitor_metrics=[f1_weighted], train_params=[f1_weighted_train_params], val_params=[f1_weighted_val_params])\n",
        "        ]\n",
        "\n",
        "    start = time.perf_counter()\n",
        "\n",
        "    history=model.fit(train_data, em_train, batch_size=batch_size, epochs=100, validation_data=(val_data, em_val, val_sample_weights), callbacks=callbacks, sample_weight=train_sample_weights)\n",
        "\n",
        "    elapsed = time.perf_counter() - start\n",
        "\n",
        "    # save log with learning rate, optimizer, batch size, conv_length, f1, acc\n",
        "    log_txt = 'optimizer: ' + str(optimizer.get_config()) + '\\n' + 'batch_size: ' +'\\n' + str(batch_size) + '\\n' + 'conv_length: ' + '\\n' + str(conv_length) + '\\n' + 'BEST train categorical_accuracy: '+'\\n' + str(np.max(history.history['categorical_accuracy']))+ '\\n' + 'BEST train f1_weighted: ' +'\\n' + str(np.max(history.history['f1_weighted'])) +'\\n'  + 'BEST val categorical_accuracy: '+'\\n' + str(np.max(history.history['val_categorical_accuracy']))+ '\\n' + 'BEST val f1_weighted: ' +'\\n' + str(np.max(history.history['val_f1_weighted'])) +'\\n'+ 'elapsed_time: '+'\\n' + str(elapsed)\n",
        "    log_txt += '\\n'+'additional info: ' + ''\n",
        "    log_file = open(f\"/content/gdrive/MyDrive/NLP_models_ArchAndWeights/log_{model_name}.txt\", \"w\")\n",
        "    log_file.write(log_txt)\n",
        "    log_file.close()\n",
        "\n",
        "else:\n",
        "\n",
        "  model.load_weights(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S74XKFo-Cvgt"
      },
      "outputs": [],
      "source": [
        "metric = \"loss\"\n",
        "if override_model:\n",
        "    plt.figure()\n",
        "    plt.plot(history.history[metric])\n",
        "    plt.plot(history.history[\"val_\" + metric])\n",
        "    plt.title(\"model \" + metric)\n",
        "    plt.ylabel(metric, fontsize=\"large\")\n",
        "    plt.xlabel(\"epoch\", fontsize=\"large\")\n",
        "    plt.legend([\"train\", \"val\"], loc=\"best\")\n",
        "    plt.yscale('log')\n",
        "    with open(os.path.join(models_folder, f'{model_name}_{metric}.pickle'), 'wb') as f:\n",
        "        fig = plt.gcf()\n",
        "        pickle.dump(fig, f)\n",
        "else:\n",
        "    with open(os.path.join(models_folder, f'{model_name}_{metric}.pickle'), 'rb') as f:\n",
        "        fig = pickle.load(f)\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBf7CgoH6809"
      },
      "outputs": [],
      "source": [
        "metric = \"categorical_accuracy\"\n",
        "if override_model:\n",
        "    plt.figure()\n",
        "    plt.plot(history.history[metric])\n",
        "    plt.plot(history.history[\"val_\" + metric])\n",
        "    plt.title(\"model \" + metric)\n",
        "    plt.ylabel(metric, fontsize=\"large\")\n",
        "    plt.xlabel(\"epoch\", fontsize=\"large\")\n",
        "    plt.legend([\"train\", \"val\"], loc=\"best\")\n",
        "    with open(os.path.join(models_folder, f'{model_name}_{metric}.pickle'), 'wb') as f:\n",
        "        fig = plt.gcf()\n",
        "        pickle.dump(fig, f)\n",
        "else:\n",
        "    with open(os.path.join(models_folder, f'{model_name}_{metric}.pickle'), 'rb') as f:\n",
        "        fig = pickle.load(f)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1gGXcSbBIgp"
      },
      "outputs": [],
      "source": [
        "metric = \"f1_weighted\"\n",
        "if override_model:\n",
        "    plt.figure()\n",
        "    plt.plot(history.history[metric])\n",
        "    plt.plot(history.history[\"val_\" + metric])\n",
        "    plt.title(\"model \" + metric)\n",
        "    plt.ylabel(metric, fontsize=\"large\")\n",
        "    plt.xlabel(\"epoch\", fontsize=\"large\")\n",
        "    plt.legend([\"train\", \"val\"], loc=\"best\")\n",
        "    with open(os.path.join(models_folder, f'{model_name}_{metric}.pickle'), 'wb') as f:\n",
        "        fig = plt.gcf()\n",
        "        pickle.dump(fig, f)\n",
        "else:\n",
        "    with open(os.path.join(models_folder, f'{model_name}_{metric}.pickle'), 'rb') as f:\n",
        "        fig = pickle.load(f)\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing resulting metrics on Validation and Test Sets"
      ],
      "metadata": {
        "id": "rCS5DHPONYmQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ab-X8gYmVtg"
      },
      "outputs": [],
      "source": [
        "# VALIDATION EVALUATION\n",
        "\n",
        "# F1\n",
        "\n",
        "f1s_metric = Masked_F1(len(label2id), mode='none', name='f1s', ignore_indexes=[label2id[emo] for emo in ignore_emotions])\n",
        "f1_macro = Masked_F1(len(label2id), mode='macro', name='f1_macro', ignore_indexes=[label2id[emo] for emo in ignore_emotions])\n",
        "val_f1_weighted_metric = Masked_F1(len(label2id), mode='weighted', name='val_f1_weighted', ignore_indexes=[label2id[emo] for emo in ignore_emotions], dataset_distribution=list(val_distr.values()))\n",
        "\n",
        "val_preds = model.predict(val_data, batch_size=batch_size, verbose=0)\n",
        "\n",
        "f1s_metric.update_state(em_val, val_preds)\n",
        "f1_macro.update_state(em_val, val_preds)\n",
        "val_f1_weighted_metric.update_state(em_val, val_preds)\n",
        "\n",
        "val_f1s = f1s_metric.result()\n",
        "val_f1_macro = f1_macro.result()\n",
        "val_f1_weighted = val_f1_weighted_metric.result()\n",
        "\n",
        "f1s_metric.reset_state()\n",
        "f1_macro.reset_state()\n",
        "val_f1_weighted_metric.reset_state()\n",
        "\n",
        "# Categorical Accuracy\n",
        "\n",
        "val_accuracy_metric = Masked_Categorical_Accuracy(ignore_indexes=[label2id[emo] for emo in ignore_emotions])\n",
        "\n",
        "val_accuracy_metric.update_state(em_val, val_preds)\n",
        "\n",
        "val_accuracy = val_accuracy_metric.result()\n",
        "\n",
        "val_accuracy_metric.reset_state()\n",
        "\n",
        "# TEST EVALUATION\n",
        "\n",
        "# F1\n",
        "\n",
        "test_f1_weighted_metric = Masked_F1(len(label2id), mode='weighted', name='test_f1_weighted', ignore_indexes=[label2id[emo] for emo in ignore_emotions], dataset_distribution=list(test_distr.values()))\n",
        "\n",
        "start = time.time()\n",
        "test_preds = model.predict(test_data, batch_size=batch_size, verbose=0)\n",
        "end = time.time()\n",
        "\n",
        "f1s_metric.update_state(em_test, test_preds)\n",
        "f1_macro.update_state(em_test, test_preds)\n",
        "test_f1_weighted_metric.update_state(em_test, test_preds)\n",
        "\n",
        "test_f1s = f1s_metric.result()\n",
        "test_f1_macro = f1_macro.result()\n",
        "test_f1_weighted = test_f1_weighted_metric.result()\n",
        "\n",
        "f1s_metric.reset_state()\n",
        "f1_macro.reset_state()\n",
        "test_f1_weighted_metric.reset_state()\n",
        "\n",
        "# Categorical Accuracy\n",
        "\n",
        "test_accuracy_metric = Masked_Categorical_Accuracy(ignore_indexes=[label2id[emo] for emo in ignore_emotions])\n",
        "\n",
        "test_accuracy_metric.update_state(em_test, test_preds)\n",
        "\n",
        "test_accuracy = test_accuracy_metric.result()\n",
        "\n",
        "test_accuracy_metric.reset_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exporting results log"
      ],
      "metadata": {
        "id": "GZobHKUpXuGe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrhgB0lt2e4U"
      },
      "outputs": [],
      "source": [
        "# OUTPUT LOG FILE\n",
        "\n",
        "log_txt = 'Validation f1s:'\n",
        "log_txt += '\\n'\n",
        "log_txt += f'{val_f1s.numpy()}'\n",
        "log_txt += '\\n'\n",
        "log_txt += 'val f1 macro:'\n",
        "log_txt += '\\n'\n",
        "log_txt += f'{val_f1_macro}'\n",
        "log_txt += '\\n'\n",
        "log_txt += 'val f1 weighted:'\n",
        "log_txt += '\\n'\n",
        "log_txt += f'{val_f1_weighted}'\n",
        "log_txt += '\\n'\n",
        "log_txt += 'Test f1s:'\n",
        "log_txt += '\\n'\n",
        "log_txt += f'{test_f1s.numpy()}'\n",
        "log_txt += '\\n'\n",
        "log_txt += 'test f1 macro:'\n",
        "log_txt += '\\n'\n",
        "log_txt += f'{test_f1_macro}'\n",
        "log_txt += '\\n'\n",
        "log_txt += 'test f1 weighted:'\n",
        "log_txt += '\\n'\n",
        "log_txt += f'{test_f1_weighted}'\n",
        "log_txt += '\\n'\n",
        "log_txt += 'test prediction time:'\n",
        "log_txt += '\\n'\n",
        "log_txt += f'{end - start}'\n",
        "log_txt += '\\n'\n",
        "log_txt += 'val categorical accuracy:'\n",
        "log_txt += '\\n'\n",
        "log_txt += f'{val_accuracy}'\n",
        "log_txt += '\\n'\n",
        "log_txt += 'test categorical accuracy:'\n",
        "log_txt += '\\n'\n",
        "log_txt += f'{test_accuracy}'\n",
        "\n",
        "\n",
        "\n",
        "log_file = open(f\"/content/gdrive/MyDrive/NLP_models_ArchAndWeights/log_{model_name}_results.txt\", \"w\")\n",
        "log_file.write(log_txt)\n",
        "log_file.close()\n",
        "\n",
        "print(log_txt)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "v1bxMwrKkhfp"
      ],
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}