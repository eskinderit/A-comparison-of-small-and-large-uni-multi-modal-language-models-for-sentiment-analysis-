{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QesmEZMEbqu9"
      },
      "source": [
        "## Downloading IEMOCAP and extracting it"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons\n",
        "!pip3 install -q git+https://github.com/vasudevgupta7/gsoc-wav2vec2@main\n",
        "!sudo apt-get install -y libsndfile1-dev\n",
        "!pip3 install -q SoundFile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DwksNdojADP",
        "outputId": "5312dd2b-0e4c-482b-db8e-5ca4e9be2a6c"
      },
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.20.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libsndfile1-dev is already the newest version (1.0.28-7ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "metadata": {
        "id": "2TI_Dp2zIH-B"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio, clear_output\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "\n",
        "files.upload()\n",
        "!ls -lha kaggle.json\n",
        "!pip install -q kaggle # Install kaggle API\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d riccardopaolini/nlp-project-work\n",
        "!unzip nlp-project-work.zip\n",
        "clear_output()\n",
        "\n",
        "folder = os.path.join(os.getcwd(), 'IEMOCAP')\n",
        "\n",
        "conv_id = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQG3B0lGbwK_",
        "outputId": "92a3ea80-0467-4b5e-89c2-ef6a98a4e452"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing Audios: 48\n",
            "Missing Sentences:\n",
            "3854    [LAUGHTER], That's what they say.\n",
            "3866                            Mmm, Hmm.\n",
            "3880                                Yeah.\n",
            "3898                               Kelly.\n",
            "3915                           Yeah, man.\n",
            "3939                      Uh-huh, uh-huh.\n",
            "3961                              Uh-huh.\n",
            "3968                                Yeah.\n",
            "3972                                Yeah.\n",
            "4010                       Well, I don't-\n",
            "4044                                Yeah.\n",
            "4827                        But, Listen--\n",
            "4847                                Yeah.\n",
            "4873                                Yeah.\n",
            "4975                          We- I mean-\n",
            "4991                                Okay.\n",
            "5005                                Yeah.\n",
            "5051                              Thanks.\n",
            "5124                        to start off.\n",
            "5181                                okay.\n",
            "5192                                Okay.\n",
            "5208                                Okay.\n",
            "7893                                  No.\n",
            "7949                             And um--\n",
            "7983                                     \n",
            "8051                      Actually, but--\n",
            "8062                               yeah--\n",
            "8082                                Yeah.\n",
            "8105                                  No.\n",
            "8106                                  No.\n",
            "8125                                Yeah.\n",
            "8181                Well, of course not--\n",
            "8543                            Yep, So--\n",
            "8778                               Augie.\n",
            "8845                                  No.\n",
            "9102                I don't know what to-\n",
            "9155                               uh-huh\n",
            "9174                                 Awe.\n",
            "9190                         right right.\n",
            "9341                                Yeah.\n",
            "9344                                Yeah.\n",
            "9354                                Yeah.\n",
            "9436                                Okay.\n",
            "9452                               Right.\n",
            "9457                             Oh good.\n",
            "9480                                Okay.\n",
            "9507                                Okay.\n",
            "9891                                  No.\n",
            "Name: sentence, dtype: object\n"
          ]
        }
      ],
      "source": [
        "df = []\n",
        "for session in ['Session1','Session2','Session3','Session4','Session5']:\n",
        "    session_path = os.path.join(folder, session)\n",
        "    # 'dialogue' folder contains Emotions and Transcripts\n",
        "    # 'sentences' folder contains Audios\n",
        "\n",
        "    trans_folder = os.path.join(session_path, 'dialog', 'transcriptions')\n",
        "\n",
        "    for trans_name in np.sort(os.listdir(trans_folder)):\n",
        "        if trans_name[:2] != '._':\n",
        "            emo_path = os.path.join(session_path, 'dialog', 'EmoEvaluation', trans_name)\n",
        "            with open(os.path.join(trans_folder, trans_name), encoding='utf8') as trans_file, open(emo_path, encoding='utf8') as emo_file:\n",
        "                conv_id += 1\n",
        "                turn_id = 0\n",
        "                for line in trans_file:\n",
        "                    audio_name, text = line.split(':')\n",
        "                    if trans_name.split('.')[0] in audio_name:\n",
        "                        turn_id += 1\n",
        "\n",
        "                        wav_path = os.path.join(session_path, 'sentences', 'wav', trans_name.split('.')[0], audio_name.split(' ')[0] + '.wav')\n",
        "\n",
        "                        reached = False\n",
        "                        count_em = {'Anger': 0, 'Happiness': 0, 'Sadness': 0, 'Neutral': 0, 'Frustration': 0, 'Excited': 0, 'Fear': 0, 'Surprise': 0, 'Disgust': 0, 'Other': 0}\n",
        "                        for emo_line in emo_file:\n",
        "                            if audio_name.split(' ')[0] in emo_line:\n",
        "                                emotion, vad = emo_line.split('\\t')[-2:]\n",
        "                                vad = vad[1:-2].split(',')\n",
        "                                reached = True\n",
        "                            elif emo_line[0] == 'C' and reached:\n",
        "                                evaluator = emo_line.split(':')[0]\n",
        "                                emotions = emo_line.split(':')[1].split('(')[0].split(';')\n",
        "                                emotions = [em.strip() for em in emotions]\n",
        "                                for em in emotions:\n",
        "                                    if em != '':\n",
        "                                        count_em[em] += 1\n",
        "                            elif reached:\n",
        "                                emo_file.seek(0)\n",
        "                                break\n",
        "                                    \n",
        "\n",
        "                        row = {'session_id': int(session[-1]),\n",
        "                                'conv_id': conv_id, \n",
        "                                'turn_id': turn_id, \n",
        "                                'sentence': text.strip(),\n",
        "                                'path': wav_path,\n",
        "                                'emotion': emotion,\n",
        "                                'valence': float(vad[0]),\n",
        "                                'activation': float(vad[1]),\n",
        "                                'dominance': float(vad[2])\n",
        "                                }\n",
        "                        \n",
        "                        df.append(dict(**row, **count_em))\n",
        "\n",
        "df = pd.DataFrame(df)\n",
        "\n",
        "idx = np.array([os.path.exists(path) for path in df.path])\n",
        "print(f'Missing Audios: {np.sum(~idx)}')\n",
        "print('Missing Sentences:')\n",
        "print(df.iloc[~idx,3])\n",
        "df = df.iloc[idx, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 274,
      "metadata": {
        "id": "dT2EsCPRWPwZ"
      },
      "outputs": [],
      "source": [
        "# selecting just first rows to avoid memory crash\n",
        "df=df.head(1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWk8nL6Ui-_0"
      },
      "source": [
        "## Setting Up\n",
        "\n",
        "Before running this notebook, please ensure that you are on GPU runtime (`Runtime` > `Change runtime type` > `GPU`). The following cell will install [`gsoc-wav2vec2`](https://github.com/vasudevgupta7/gsoc-wav2vec2) package & its dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 274,
      "metadata": {
        "id": "HvGGOgq2Upkf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvuJL8-f0zn5"
      },
      "source": [
        "## Model setup using `TFHub`\n",
        "\n",
        "We will start by importing some libraries/modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3_fgx4eZvM7",
        "outputId": "9de9b8d1-3725-46fe-a3b5-82ff42701106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF version: 2.12.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "print(\"TF version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0rVUxyWsS5f"
      },
      "source": [
        "First, we will download our model from TFHub & will wrap our model signature with [`hub.KerasLayer`](https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer) to be able to use this model like any other Keras layer. Fortunately, `hub.KerasLayer` can do both in just 1 line.\n",
        "\n",
        "**Note:** When loading model with `hub.KerasLayer`, model becomes a bit opaque but sometimes we need finer controls over the model, then we can load the model with `tf.keras.models.load_model(...)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "metadata": {
        "id": "NO6QRC7KZvM9"
      },
      "outputs": [],
      "source": [
        "pretrained_layer = hub.KerasLayer(\"https://tfhub.dev/vasudevgupta7/wav2vec2/1\", trainable=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCputyVBv2e9"
      },
      "source": [
        "You can refer to this [script](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/export2hub.py) in case you are interested in the model exporting script. Object `pretrained_layer` is the freezed version of [`Wav2Vec2Model`](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/wav2vec2/modeling.py). These pre-trained weights were converted from HuggingFace PyTorch [pre-trained weights](https://huggingface.co/facebook/wav2vec2-base) using [this script](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/convert_torch_to_tf.py).\n",
        "\n",
        "Originally, wav2vec2 was pre-trained with a masked language modelling approach with the objective to identify the true quantized latent speech representation for a masked time step. You can read more about the training objective in the paper- [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SseDnCr7hyhC"
      },
      "source": [
        "Now, we will be defining a few constants and hyper-parameters which will be useful in the next few cells. `AUDIO_MAXLEN` is intentionally set to `246000` as the model signature only accepts static sequence length of `246000`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "metadata": {
        "id": "eiILuMBERxlO"
      },
      "outputs": [],
      "source": [
        "AUDIO_MAXLEN = 246000\n",
        "LABEL_MAXLEN = 11\n",
        "BATCH_SIZE = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V4gTgGLgXvO"
      },
      "source": [
        "In the following cell, we will wrap `pretrained_layer` & a dense layer (LM head) with the [Keras's Functional API](https://www.tensorflow.org/guide/keras/functional)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "metadata": {
        "id": "a3CUN1KEB10Q"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "IEMOCAP_CLASSES = LABEL_MAXLEN\n",
        "#change output classes config.vocab_size\n",
        "\n",
        "inputs = tf.keras.Input(shape=(AUDIO_MAXLEN,))\n",
        "hidden_states = pretrained_layer(inputs)\n",
        "out = tf.keras.layers.GlobalMaxPool1D()(hidden_states)\n",
        "out = Dense(128, activation='relu')(out)\n",
        "out = Dropout(0.1)(out)\n",
        "out = Dense(32, activation = 'relu')(out)\n",
        "outputs = Dense(IEMOCAP_CLASSES, activation = 'Softmax')(out)\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zDXuoMXhDMo"
      },
      "source": [
        "The dense layer (defined above) is having an output dimension of `vocab_size` as we want to predict probabilities of each token in the vocabulary at each time step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPp18ZHRtnq-"
      },
      "source": [
        "## Setting up training state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATQy1ZK3vFr7"
      },
      "source": [
        "In TensorFlow, model weights are built only when `model.call` or `model.build` is called for the first time, so the following cell will build the model weights for us. Further, we will be running `model.summary()` to check the total number of trainable parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 279,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgL5wyaXZvM-",
        "outputId": "1108b233-75dd-4a18-d49a-78855be5101e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_12 (InputLayer)       [(None, 246000)]          0         \n",
            "                                                                 \n",
            " keras_layer_9 (KerasLayer)  (None, 768, 768)          94371712  \n",
            "                                                                 \n",
            " global_max_pooling1d_7 (Glo  (None, 768)              0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 128)               98432     \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 32)                4128      \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 11)                363       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 94,474,635\n",
            "Trainable params: 94,474,635\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model(tf.random.uniform(shape=(BATCH_SIZE, AUDIO_MAXLEN)))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQxxA4Fevp7m"
      },
      "source": [
        "Now, we need to define the `loss_fn` and optimizer to be able to train the model. The following cell will do that for us. We will be using the `Adam` optimizer for simplicity. `CTCLoss` is a common loss type that is used for tasks (like `ASR`) where input sub-parts can't be easily aligned with output sub-parts. You can read more about CTC-loss from this amazing [blog post](https://distill.pub/2017/ctc/).\n",
        "\n",
        "\n",
        "`CTCLoss` (from [`gsoc-wav2vec2`](https://github.com/vasudevgupta7/gsoc-wav2vec2) package) accepts 3 arguments: `config`, `model_input_shape` & `division_factor`. If `division_factor=1`, then loss will simply get summed, so pass `division_factor` accordingly to get mean over batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 280,
      "metadata": {
        "id": "glDepVEHZvM_"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "\n",
        "loss_fn = 'categorical_crossentropy',\n",
        "\n",
        "LEARNING_RATE = 5e-5\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mvTuOXpwsQe"
      },
      "source": [
        "## Loading & Pre-processing data\n",
        "\n",
        "Let's now download the LibriSpeech dataset from the [official website](http://www.openslr.org/12) and set it up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 281,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkIu_Wt4ZvNA",
        "outputId": "388c0c65-dba9-4887-8645-1bcff3165d6d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/IEMOCAP/Session1/sentences/wav/Ses01F_impro01/Ses01F_impro01_F000.wav',\n",
              " '/content/IEMOCAP/Session1/sentences/wav/Ses01F_impro01/Ses01F_impro01_M000.wav',\n",
              " '/content/IEMOCAP/Session1/sentences/wav/Ses01F_impro01/Ses01F_impro01_F001.wav',\n",
              " '/content/IEMOCAP/Session1/sentences/wav/Ses01F_impro01/Ses01F_impro01_M001.wav']"
            ]
          },
          "metadata": {},
          "execution_count": 281
        }
      ],
      "source": [
        "flac_files = df.iloc[:,4].to_list()\n",
        "flac_files[:4]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEObi_Apk3ZD"
      },
      "source": [
        "Alright, so each sub-directory has many `.flac` files and a `.txt` file. The `.txt` file contains text transcriptions for all the speech samples (i.e. `.flac` files) present in that sub-directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ldkf_ceb0_YW"
      },
      "source": [
        "Similarly, we will define a function for loading a speech sample from a `.flac` file.\n",
        "\n",
        "`REQUIRED_SAMPLE_RATE` is set to `16000` as wav2vec2 was pre-trained with `16K` frequency and it's recommended to fine-tune it without any major change in data distribution due to frequency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 282,
      "metadata": {
        "id": "YOJ3OzPsTyXv"
      },
      "outputs": [],
      "source": [
        "import soundfile as sf\n",
        "\n",
        "REQUIRED_SAMPLE_RATE = 16000\n",
        "\n",
        "def read_flac_file(file_path):\n",
        "  with open(file_path, \"rb\") as f:\n",
        "      audio, sample_rate = sf.read(f)\n",
        "  if sample_rate != REQUIRED_SAMPLE_RATE:\n",
        "      raise ValueError(\n",
        "          f\"sample rate (={sample_rate}) of your files must be {REQUIRED_SAMPLE_RATE}\"\n",
        "      )\n",
        "  file_id = os.path.split(file_path)[-1][:-len(\".wav\")]\n",
        "  return audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sxDN8P4nWkW"
      },
      "source": [
        "Now, we will pick some random samples & will try to visualize them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {
        "id": "HI5J-2Dfm_wT"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "import random\n",
        "\n",
        "file_id = random.choice([f[:-len(\".wav\")] for f in flac_files])\n",
        "flac_file_path = os.path.join(f\"{file_id}.wav\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8jJ7Ed81p_A"
      },
      "source": [
        "Now, we will combine all the speech & text samples and will define the function (in next cell) for that purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {
        "id": "MI-5YCzaTsei"
      },
      "outputs": [],
      "source": [
        "def fetch_sound_text_mapping(flac_paths, emotions):\n",
        "\n",
        "  speech_samples = []\n",
        "  for f in flac_paths:\n",
        "    speech_samples.append(read_flac_file(f))\n",
        "  \n",
        "  assert len(emotions) == len(speech_samples)\n",
        "\n",
        "  samples = [(speech_samples[file_id], emotions[file_id]) for file_id in range(len(emotions)) if len(speech_samples[file_id]) < AUDIO_MAXLEN]\n",
        "\n",
        "  return samples\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx95Lxvu0nT4"
      },
      "source": [
        "It's time to have a look at a few samples ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ls7X_jqIz4R",
        "outputId": "c932f582-40d7-4e2b-8099-95b24043b762"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(array([-0.0050354 , -0.00497437, -0.0038147 , ..., -0.00265503,\n",
              "         -0.00317383, -0.00418091]),\n",
              "  'neu'),\n",
              " (array([-0.00354004, -0.00308228, -0.0062561 , ...,  0.00341797,\n",
              "          0.00274658,  0.00256348]),\n",
              "  'fru'),\n",
              " (array([ 0.00094604, -0.00094604, -0.0007019 , ..., -0.00045776,\n",
              "         -0.00033569, -0.00128174]),\n",
              "  'neu'),\n",
              " (array([ 0.00021362, -0.00048828, -0.00143433, ..., -0.00378418,\n",
              "         -0.00375366, -0.00292969]),\n",
              "  'fru'),\n",
              " (array([-0.00036621, -0.00015259,  0.00042725, ..., -0.00030518,\n",
              "         -0.00018311,  0.00088501]),\n",
              "  'neu')]"
            ]
          },
          "metadata": {},
          "execution_count": 285
        }
      ],
      "source": [
        "samples = fetch_sound_text_mapping(flac_files, df.emotion.to_list())\n",
        "samples[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUjhSWfsnlCL"
      },
      "source": [
        "Note: We are loading this data into memory as we working with a small amount of dataset in this notebook. But for training on the complete dataset (~300 GBs), you will have to load data lazily. You can refer to [this script](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/data_utils.py) to know more on that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg8Zia1kzw0J"
      },
      "source": [
        "Let's pre-process the data now !!!\n",
        "\n",
        "We will first define the tokenizer & processor using `gsoc-wav2vec2` package. Then, we will do very simple pre-processing. `processor` will normalize raw speech w.r.to frames axis and `tokenizer` will convert our model outputs into the string (using the defined vocabulary) & will take care of the removal of special tokens (depending on your tokenizer configuration)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 286,
      "metadata": {
        "id": "gaat_hMLNVHF"
      },
      "outputs": [],
      "source": [
        "from wav2vec2 import Wav2Vec2Processor\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "#tokenizer = Wav2Vec2Processor(is_tokenizer=True)\n",
        "processor = Wav2Vec2Processor(is_tokenizer=False)\n",
        "\n",
        "def preprocess_emotions(emotion):\n",
        "  # tokenize function for emotions\n",
        "  encoded_dict= {'ang':0, 'dis':1, 'exc':2, 'fea':3, 'fru':4, 'hap':5,'neu':6, 'oth':7, 'sad':8, 'sur':9, 'xxx':10}\n",
        "  label=to_categorical(encoded_dict.get(emotion), num_classes=11)\n",
        "  return tf.constant(label, dtype=tf.int32)\n",
        "\n",
        "def preprocess_speech(audio):\n",
        "  # tokenize function for audio\n",
        "  audio = tf.constant(audio, dtype=tf.float32)\n",
        "  return processor(tf.transpose(audio))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(preprocess_emotions('ang'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jbpC2wsmG3T",
        "outputId": "df0f5811-f007-4010-88cc-de2125e6c1e0"
      },
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([1 0 0 0 0 0 0 0 0 0 0], shape=(11,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyKl8QP-zRFC"
      },
      "source": [
        "Now, we will define the python generator to call the preprocessing functions we defined in above cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 288,
      "metadata": {
        "id": "PoQrRalwMpQ6"
      },
      "outputs": [],
      "source": [
        "## emotions instead of text, change samples\n",
        "def inputs_generator():\n",
        "  for speech, emotion in samples:\n",
        "    yield preprocess_speech(speech), preprocess_emotions(emotion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Vlm3ySFULsG"
      },
      "source": [
        "## Setting up `tf.data.Dataset`\n",
        "\n",
        "Following cell will setup `tf.data.Dataset` object using its `.from_generator(...)` method. We will be using the `generator` object, we defined in the above cell.\n",
        "\n",
        "**Note:** For distributed training (especially on TPUs), `.from_generator(...)` doesn't work currently and it is recommended to train on data stored in `.tfrecord` format (Note: The TFRecords should ideally be stored inside a GCS Bucket in order for the TPUs to work to the fullest extent).\n",
        "\n",
        "You can refer to [this script](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/make_tfrecords.py) for more details on how to convert LibriSpeech data into tfrecords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 289,
      "metadata": {
        "id": "LbQ_dMwGO62h"
      },
      "outputs": [],
      "source": [
        "output_signature = (\n",
        "    tf.TensorSpec(shape=(None),  dtype=tf.float32),\n",
        "    tf.TensorSpec(shape=(None), dtype=tf.int32),\n",
        ")\n",
        "\n",
        "dataset = tf.data.Dataset.from_generator(inputs_generator, output_signature=output_signature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 290,
      "metadata": {
        "id": "HXBbNsRyPyw3"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = len(flac_files)\n",
        "SEED = 42\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE, seed=SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DAUmns3pXfr"
      },
      "source": [
        "We will pass the dataset into multiple batches, so let's prepare batches in the following cell. Now, all the sequences in a batch should be padded to a constant length. We will use the`.padded_batch(...)` method for that purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 291,
      "metadata": {
        "id": "Okhko1IWRida"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.padded_batch(BATCH_SIZE, padded_shapes=(AUDIO_MAXLEN, LABEL_MAXLEN), padding_values=(0.0, 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A45CjQG5qSbV"
      },
      "source": [
        "Accelerators (like GPUs/TPUs) are very fast and often data-loading (& pre-processing) becomes the bottleneck during training as the data-loading part happens on CPUs. This can increase the training time significantly especially when there is a lot of online pre-processing involved or data is streamed online from GCS buckets. To handle those issues, `tf.data.Dataset` offers the `.prefetch(...)` method. This method helps in preparing the next few batches in parallel (on CPUs) while the model is making predictions (on GPUs/TPUs) on the current batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 292,
      "metadata": {
        "id": "f-bKu2YjRior"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqk2cs6LxVIh"
      },
      "source": [
        "Since this notebook is made for demonstration purposes, we will be taking first `num_train_batches` and will perform training over only that. You are encouraged to train on the whole dataset though. Similarly, we will evaluate only `num_val_batches`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 293,
      "metadata": {
        "id": "z6GO5oYUxXtz"
      },
      "outputs": [],
      "source": [
        "num_train_batches = 400 # x batch size (2)\n",
        "num_val_batches = 100 # x batch size (2)\n",
        "\n",
        "train_dataset = dataset.take(num_train_batches)\n",
        "val_dataset = dataset.skip(num_train_batches).take(num_val_batches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzAOI78tky08"
      },
      "source": [
        "## Model training\n",
        "\n",
        "For training our model, we will be directly calling `.fit(...)` method after compiling our model with `.compile(...)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "metadata": {
        "id": "vuBY2sZElgwg"
      },
      "outputs": [],
      "source": [
        "f1 = tfa.metrics.F1Score(num_classes=IEMOCAP_CLASSES,average='weighted')\n",
        "accuracy = 'accuracy'\n",
        "\n",
        "\n",
        "model.compile(optimizer, loss=loss_fn, metrics = [accuracy,f1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPQ7L6dHHwBV",
        "outputId": "831eb0e4-304e-453c-d66c-965d4d2e0084"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2245"
            ]
          },
          "metadata": {},
          "execution_count": 295
        }
      ],
      "source": [
        "import gc\n",
        "\n",
        "del(df)\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qswxafSl0HjO"
      },
      "source": [
        "The above cell will set up our training state. Now we can initiate training with the `.fit(...)` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtuSfnj1l-I_",
        "outputId": "a0518b56-4dfd-440a-ed3a-b534d662d689"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['wav2vec2/masked_spec_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['wav2vec2/masked_spec_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['wav2vec2/masked_spec_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['wav2vec2/masked_spec_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400/400 [==============================] - 339s 715ms/step - loss: 2.0686 - accuracy: 0.2175 - f1_score: 0.1667 - val_loss: 1.9633 - val_accuracy: 0.2649 - val_f1_score: 0.1109\n",
            "Epoch 2/3\n",
            "400/400 [==============================] - ETA: 0s - loss: 1.9845 - accuracy: 0.2138 - f1_score: 0.1543"
          ]
        }
      ],
      "source": [
        "history = model.fit(train_dataset, validation_data=val_dataset, epochs=3)\n",
        "history.history"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "EoWmEIiHZ2zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(preprocess_speech(samples[0][0]))\n",
        "#model.predict(preprocess_speech(samples[0][0]))"
      ],
      "metadata": {
        "id": "0Q5KdOXYlahV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySvp8r2E1q_V"
      },
      "source": [
        "Let's save our model with `.save(...)` method to be able to perform inference later. You can also export this SavedModel to TFHub by following [TFHub documentation](https://www.tensorflow.org/hub/publish)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0KEYcwydwjF"
      },
      "outputs": [],
      "source": [
        "save_dir = \"finetuned-wav2vec2\"\n",
        "model.save(save_dir, include_optimizer=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkOpp9rZ211t"
      },
      "source": [
        "Note: We are setting `include_optimizer=False` as we want to use this model for inference only."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJfPlTgezD0i"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Now we will be computing Word Error Rate over the validation dataset\n",
        "\n",
        "**Word error rate** (WER) is a common metric for measuring the performance of an automatic speech recognition system. The WER is derived from the Levenshtein distance, working at the word level. Word error rate can then be computed as: WER = (S + D + I) / N = (S + D + I) / (S + D + C) where S is the number of substitutions, D is the number of deletions, I is the number of insertions, C is the number of correct words, N is the number of words in the reference (N=S+D+C). This value indicates the percentage of words that were incorrectly predicted. \n",
        "\n",
        "You can refer to [this paper](https://www.isca-speech.org/archive_v0/interspeech_2004/i04_2765.html) to learn more about WER."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io_91Y7-r3xu"
      },
      "source": [
        "We will use `load_metric(...)` function from [HuggingFace datasets](https://huggingface.co/docs/datasets/) library. Let's first install the `datasets` library using `pip` and then define the `metric` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GW9F_oVDU1TZ"
      },
      "outputs": [],
      "source": [
        "#!pip3 install -q datasets\n",
        "\n",
        "#from datasets import load_metric\n",
        "#metric = load_metric(\"wer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssWXWc7CZvNB"
      },
      "outputs": [],
      "source": [
        "@tf.function(jit_compile=True)\n",
        "def eval_fwd(batch):\n",
        "  logits = model(batch, training=False)\n",
        "  return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFh1myg1x4ua"
      },
      "source": [
        "It's time to run the evaluation on validation data now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQTFVjZghckJ"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "for speech, labels in tqdm(val_dataset, total=num_val_batches):\n",
        "    predictions  = eval_fwd(speech)\n",
        "    print(predictions)\n",
        "    #predictions = [tokenizer.decode(pred) for pred in predictions.numpy().tolist()]\n",
        "    #references = [tokenizer.decode(label, group_tokens=False) for label in labels.numpy().tolist()]\n",
        "    #metric.add_batch(references=references, predictions=predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWCc8qBesv3e"
      },
      "source": [
        "We are using the `tokenizer.decode(...)` method for decoding our predictions and labels back into the text and will add them to the metric for `WER` computation later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI_URj8Wtb2g"
      },
      "source": [
        "Now, let's calculate the metric value in following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a83wekLgWMod"
      },
      "outputs": [],
      "source": [
        "metric.compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_cD1OgVEjl4"
      },
      "source": [
        "**Note:** Here metric value doesn't make any sense as the model is trained on very small data and ASR-like tasks often require a large amount of data to learn a mapping from speech to text. You should probably train on large data to get some good results. This notebook gives you a template to fine-tune a pre-trained speech model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G14o706kdTE1"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Now that we are satisfied with the training process & have saved the model in `save_dir`, we will see how this model can be used for inference.\n",
        "\n",
        "First, we will load our model using `tf.keras.models.load_model(...)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrTrExiUdaED"
      },
      "outputs": [],
      "source": [
        "finetuned_model = tf.keras.models.load_model(save_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luodSroz20SR"
      },
      "source": [
        "Let's download some speech samples for performing inference. You can replace the following sample with your speech sample also."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUE0shded6Ej"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/vasudevgupta7/gsoc-wav2vec2/raw/main/data/SA2.wav"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycBjU_U53FjL"
      },
      "source": [
        "Now, we will read the speech sample using `soundfile.read(...)` and pad it to `AUDIO_MAXLEN` to satisfy the model signature. Then we will normalize that speech sample using the `Wav2Vec2Processor` instance & will feed it into the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7CARje4d5_H"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "speech, _ = sf.read(\"SA2.wav\")\n",
        "speech = np.pad(speech, (0, AUDIO_MAXLEN - len(speech)))\n",
        "speech = tf.expand_dims(processor(tf.constant(speech)), 0)\n",
        "\n",
        "outputs = finetuned_model(speech)\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUSttSPa30qP"
      },
      "source": [
        "Let's decode numbers back into text sequence using the `Wav2Vec2tokenizer` instance, we defined above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYdJqxQ4llgI"
      },
      "outputs": [],
      "source": [
        "predictions = tf.argmax(outputs, axis=-1)\n",
        "predictions = [tokenizer.decode(pred) for pred in predictions.numpy().tolist()]\n",
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DXC757bztJc"
      },
      "source": [
        "This prediction is quite random as the model was never trained on large data in this notebook (as this notebook is not meant for doing complete training). You will get good predictions if you train this model on complete LibriSpeech dataset.\n",
        "\n",
        "Finally, we have reached an end to this notebook. But it's not an end of learning TensorFlow for speech-related tasks, this [repository](https://github.com/tulasiram58827/TTS_TFLite) contains some more amazing tutorials. In case you encountered any bug in this notebook, please create an issue [here](https://github.com/vasudevgupta7/gsoc-wav2vec2/issues)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBEm6caxYDyK"
      },
      "outputs": [],
      "source": [
        "read_flac_file('SA2.wav')\n",
        "Audio(filename='SA2.wav')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BI-HB6D8bn_Z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "184c0bf4d405d4a36e719b504ff2a22c838d19108535bf816dff1a5aad495b87"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit ('t5': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}