{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A72FyGvkkwXG"
      },
      "source": [
        "# Audio Classification with Hugging Face Transformers\n",
        "**author: Alessandro D'Amico**\n",
        "\n",
        "This code was adapted from an original version\n",
        "credits to *Sreyan Ghosh* for the original version of this notebook, reachable at the following link:\n",
        "\n",
        "- [ Training Wav2Vec 2.0 using Hugging Face Transformers for Audio Classification](https://github.com/keras-team/keras-io/blob/master/examples/audio/wav2vec2_audiocls.py)\n",
        "\n",
        "\n",
        "## N.B.: Requirements\n",
        "- RAM > 13 GB\n",
        "- VRAM > 15 GB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZzCkOT4kwXP"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Identification of speech commands, also known as *keyword spotting* (KWS),\n",
        "is important from an engineering perspective for a wide range of applications,\n",
        "from indexing audio databases and indexing keywords, to running speech models locally\n",
        "on microcontrollers. Currently, many human-computer interfaces (HCI) like Google\n",
        "Assistant, Microsoft Cortana, Amazon Alexa, Apple Siri and others rely on keyword\n",
        "spotting. There is a significant amount of research in the field by all major companies,\n",
        "notably Google and Baidu.\n",
        "\n",
        "In the past decade, deep learning has led to significant performance\n",
        "gains on this task. Though low-level audio features extracted from raw audio like MFCC or\n",
        "mel-filterbanks have been used for decades, the design of these low-level features\n",
        "are [flawed by biases](https://arxiv.org/abs/2101.08596). Moreover, deep learning models\n",
        "trained on these low-level features can easily overfit to noise or signals irrelevant to the\n",
        "task.  This makes it is essential for any system to learn speech representations that make\n",
        "high-level information, such as acoustic and linguistic content, including phonemes,\n",
        "words, semantic meanings, tone, speaker characteristics from speech signals available to\n",
        "solve the downstream task. [Wav2Vec 2.0](https://arxiv.org/abs/2006.11477), which solves a\n",
        "self-supervised contrastive learning task to learn high-level speech representations,\n",
        "provides a great alternative to traditional low-level features for training deep learning\n",
        "models for KWS.\n",
        "\n",
        "In this notebook, we train the Wav2Vec 2.0 to recognize the leading emotion, using IEMOCAP (audio files) as dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing IEMOCAP"
      ],
      "metadata": {
        "id": "jcnlWnBmt2FN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons\n",
        "!pip3 install -q git+https://github.com/vasudevgupta7/gsoc-wav2vec2@main\n",
        "!sudo apt-get install -y libsndfile1-dev\n",
        "!pip3 install -q SoundFile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMiwft2Ut4If",
        "outputId": "9c5d2301-1f05-4ccb-fafa-dbe481635381"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.20.0 typeguard-2.13.3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.3/203.3 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for wav2vec2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libsndfile1-dev is already the newest version (1.0.28-7ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Downloading IEMOCAP from private repository (using `kaggle.json` API Key for enabled users)"
      ],
      "metadata": {
        "id": "-MWfZf-F3rMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio, clear_output\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow_addons as tfa\n",
        "from IPython.display import Audio\n",
        "import random\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "from transformers import AutoFeatureExtractor\n",
        "\n",
        "\n",
        "\n",
        "files.upload()\n",
        "!ls -lha kaggle.json\n",
        "!pip install -q kaggle # Install kaggle API\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d riccardopaolini/nlp-project-work\n",
        "!unzip nlp-project-work.zip\n",
        "clear_output()\n",
        "\n",
        "folder = os.path.join(os.getcwd(), 'IEMOCAP')\n",
        "\n",
        "conv_id = 0"
      ],
      "metadata": {
        "id": "xhGmEzBGCmZn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building IEMOCAP Pandas DataFrame"
      ],
      "metadata": {
        "id": "rhozeQJO4YYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = []\n",
        "for session in ['Session1','Session2','Session3','Session4','Session5']:\n",
        "    session_path = os.path.join(folder, session)\n",
        "    # 'dialogue' folder contains Emotions and Transcripts\n",
        "    # 'sentences' folder contains Audios\n",
        "\n",
        "    trans_folder = os.path.join(session_path, 'dialog', 'transcriptions')\n",
        "\n",
        "    for trans_name in np.sort(os.listdir(trans_folder)):\n",
        "        if trans_name[:2] != '._':\n",
        "            emo_path = os.path.join(session_path, 'dialog', 'EmoEvaluation', trans_name)\n",
        "            with open(os.path.join(trans_folder, trans_name), encoding='utf8') as trans_file, open(emo_path, encoding='utf8') as emo_file:\n",
        "                conv_id += 1\n",
        "                turn_id = 0\n",
        "                for line in trans_file:\n",
        "                    audio_name, text = line.split(':')\n",
        "                    if trans_name.split('.')[0] in audio_name:\n",
        "                        turn_id += 1\n",
        "\n",
        "                        wav_path = os.path.join(session_path, 'sentences', 'wav', trans_name.split('.')[0], audio_name.split(' ')[0] + '.wav')\n",
        "\n",
        "                        reached = False\n",
        "                        count_em = {'Anger': 0, 'Happiness': 0, 'Sadness': 0, 'Neutral': 0, 'Frustration': 0, 'Excited': 0, 'Fear': 0, 'Surprise': 0, 'Disgust': 0, 'Other': 0}\n",
        "                        for emo_line in emo_file:\n",
        "                            if audio_name.split(' ')[0] in emo_line:\n",
        "                                emotion, vad = emo_line.split('\\t')[-2:]\n",
        "                                vad = vad[1:-2].split(',')\n",
        "                                reached = True\n",
        "                            elif emo_line[0] == 'C' and reached:\n",
        "                                evaluator = emo_line.split(':')[0]\n",
        "                                emotions = emo_line.split(':')[1].split('(')[0].split(';')\n",
        "                                emotions = [em.strip() for em in emotions]\n",
        "                                for em in emotions:\n",
        "                                    if em != '':\n",
        "                                        count_em[em] += 1\n",
        "                            elif reached:\n",
        "                                emo_file.seek(0)\n",
        "                                break\n",
        "                                    \n",
        "\n",
        "                        row = {'session_id': int(session[-1]),\n",
        "                                'conv_id': conv_id, \n",
        "                                'turn_id': turn_id, \n",
        "                                'sentence': text.strip(),\n",
        "                                'path': wav_path,\n",
        "                                'emotion': emotion,\n",
        "                                'valence': float(vad[0]),\n",
        "                                'activation': float(vad[1]),\n",
        "                                'dominance': float(vad[2])\n",
        "                                }\n",
        "                        \n",
        "                        df.append(dict(**row, **count_em))\n",
        "\n",
        "df = pd.DataFrame(df)\n",
        "\n",
        "idx = np.array([os.path.exists(path) for path in df.path])\n",
        "print(f'Missing Audios: {np.sum(~idx)}')\n",
        "print('Missing Sentences:')\n",
        "print(df.iloc[~idx,3])\n",
        "df = df.iloc[idx, :]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcJBdeDMCqGS",
        "outputId": "4d873fa0-883b-430b-95e1-2bcc883e5067"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing Audios: 48\n",
            "Missing Sentences:\n",
            "3854    [LAUGHTER], That's what they say.\n",
            "3866                            Mmm, Hmm.\n",
            "3880                                Yeah.\n",
            "3898                               Kelly.\n",
            "3915                           Yeah, man.\n",
            "3939                      Uh-huh, uh-huh.\n",
            "3961                              Uh-huh.\n",
            "3968                                Yeah.\n",
            "3972                                Yeah.\n",
            "4010                       Well, I don't-\n",
            "4044                                Yeah.\n",
            "4827                        But, Listen--\n",
            "4847                                Yeah.\n",
            "4873                                Yeah.\n",
            "4975                          We- I mean-\n",
            "4991                                Okay.\n",
            "5005                                Yeah.\n",
            "5051                              Thanks.\n",
            "5124                        to start off.\n",
            "5181                                okay.\n",
            "5192                                Okay.\n",
            "5208                                Okay.\n",
            "7893                                  No.\n",
            "7949                             And um--\n",
            "7983                                     \n",
            "8051                      Actually, but--\n",
            "8062                               yeah--\n",
            "8082                                Yeah.\n",
            "8105                                  No.\n",
            "8106                                  No.\n",
            "8125                                Yeah.\n",
            "8181                Well, of course not--\n",
            "8543                            Yep, So--\n",
            "8778                               Augie.\n",
            "8845                                  No.\n",
            "9102                I don't know what to-\n",
            "9155                               uh-huh\n",
            "9174                                 Awe.\n",
            "9190                         right right.\n",
            "9341                                Yeah.\n",
            "9344                                Yeah.\n",
            "9354                                Yeah.\n",
            "9436                                Okay.\n",
            "9452                               Right.\n",
            "9457                             Oh good.\n",
            "9480                                Okay.\n",
            "9507                                Okay.\n",
            "9891                                  No.\n",
            "Name: sentence, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETLsbKyiEeeS",
        "outputId": "e6c9022a-e3c6-4357-da36-21eb2385de96"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10039, 19)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "X56sh_FUEoUW",
        "outputId": "22bae903-0441-4bd9-9cc2-9d972ffc803e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   session_id  conv_id  turn_id                 sentence  \\\n",
              "0           1        1        1               Excuse me.   \n",
              "1           1        1        2  Do you have your forms?   \n",
              "2           1        1        3                    Yeah.   \n",
              "3           1        1        4         Let me see them.   \n",
              "4           1        1        5      Is there a problem?   \n",
              "\n",
              "                                                path emotion  valence  \\\n",
              "0  /content/IEMOCAP/Session1/sentences/wav/Ses01F...     neu      2.5   \n",
              "1  /content/IEMOCAP/Session1/sentences/wav/Ses01F...     fru      2.5   \n",
              "2  /content/IEMOCAP/Session1/sentences/wav/Ses01F...     neu      2.5   \n",
              "3  /content/IEMOCAP/Session1/sentences/wav/Ses01F...     fru      2.5   \n",
              "4  /content/IEMOCAP/Session1/sentences/wav/Ses01F...     neu      2.5   \n",
              "\n",
              "   activation  dominance  Anger  Happiness  Sadness  Neutral  Frustration  \\\n",
              "0         2.5        2.5      0          0        0        4            0   \n",
              "1         2.0        2.5      0          0        0        1            3   \n",
              "2         2.5        2.5      1          0        0        4            0   \n",
              "3         2.0        2.5      0          0        0        0            3   \n",
              "4         2.5        2.5      1          0        0        3            0   \n",
              "\n",
              "   Excited  Fear  Surprise  Disgust  Other  \n",
              "0        0     0         0        0      0  \n",
              "1        0     0         0        0      1  \n",
              "2        0     0         0        0      0  \n",
              "3        0     0         0        0      1  \n",
              "4        0     0         1        0      0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-336d7ef4-db9b-40ce-9c8f-08e2baedbb0a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>session_id</th>\n",
              "      <th>conv_id</th>\n",
              "      <th>turn_id</th>\n",
              "      <th>sentence</th>\n",
              "      <th>path</th>\n",
              "      <th>emotion</th>\n",
              "      <th>valence</th>\n",
              "      <th>activation</th>\n",
              "      <th>dominance</th>\n",
              "      <th>Anger</th>\n",
              "      <th>Happiness</th>\n",
              "      <th>Sadness</th>\n",
              "      <th>Neutral</th>\n",
              "      <th>Frustration</th>\n",
              "      <th>Excited</th>\n",
              "      <th>Fear</th>\n",
              "      <th>Surprise</th>\n",
              "      <th>Disgust</th>\n",
              "      <th>Other</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Excuse me.</td>\n",
              "      <td>/content/IEMOCAP/Session1/sentences/wav/Ses01F...</td>\n",
              "      <td>neu</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>Do you have your forms?</td>\n",
              "      <td>/content/IEMOCAP/Session1/sentences/wav/Ses01F...</td>\n",
              "      <td>fru</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Yeah.</td>\n",
              "      <td>/content/IEMOCAP/Session1/sentences/wav/Ses01F...</td>\n",
              "      <td>neu</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>Let me see them.</td>\n",
              "      <td>/content/IEMOCAP/Session1/sentences/wav/Ses01F...</td>\n",
              "      <td>fru</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>Is there a problem?</td>\n",
              "      <td>/content/IEMOCAP/Session1/sentences/wav/Ses01F...</td>\n",
              "      <td>neu</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-336d7ef4-db9b-40ce-9c8f-08e2baedbb0a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-336d7ef4-db9b-40ce-9c8f-08e2baedbb0a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-336d7ef4-db9b-40ce-9c8f-08e2baedbb0a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Global variables"
      ],
      "metadata": {
        "id": "8GQ8xTbaApbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Maximum duration of the input audio file we feed to our Wav2Vec 2.0 model.\n",
        "MAX_DURATION = 1\n",
        "# Sampling rate is the number of samples of audio recorded every second\n",
        "SAMPLING_RATE = 16000\n",
        "BATCH_SIZE = 1  # Batch-size for training and evaluating our model.\n",
        "# Datasets has to be of a size that is multiple of this: // BATCH_SIZE) * BATCH_SIZE\n",
        "NUM_CLASSES = 11  # Number of classes our dataset will have (11 in our case).\n",
        "HIDDEN_DIM = 768  # Dimension of our model output (768 in case of Wav2Vec 2.0 - Base).\n",
        "MAX_SEQ_LENGTH = MAX_DURATION * SAMPLING_RATE  # Maximum length of the input audio file.\n",
        "# Wav2Vec 2.0 results in an output frequency with a stride of about 20ms.\n",
        "MAX_FRAMES = 49\n",
        "MAX_EPOCHS = 2  # Maximum number of training epochs.\n",
        "\n",
        "MODEL_CHECKPOINT = \"facebook/wav2vec2-base\"  # Name of pretrained model from Hugging Face Model Hub"
      ],
      "metadata": {
        "id": "YOP_v80eAnYU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input Preprocessing"
      ],
      "metadata": {
        "id": "Xa_fjh0s_RLk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting audio samples"
      ],
      "metadata": {
        "id": "Xgzx4UeI_JPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import soundfile as sf\n",
        "\n",
        "def read_wav_file(file_path):\n",
        "  with open(file_path, \"rb\") as f:\n",
        "      audio, sample_rate = sf.read(f)\n",
        "  if sample_rate != SAMPLING_RATE:\n",
        "      raise ValueError(\n",
        "          f\"sample rate (={sample_rate}) of your files must be {SAMPLING_RATE}\"\n",
        "      )\n",
        "  file_id = os.path.split(file_path)[-1][:-len(\".wav\")]\n",
        "  return audio #.tolist()\n",
        "\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
        "    MODEL_CHECKPOINT, return_attention_mask=True\n",
        ")\n",
        "\n",
        "\n",
        "def preprocess_function(audio_arrays):\n",
        "    #audio_arrays: audio files encoded in float nums\n",
        "    inputs = feature_extractor(\n",
        "        audio_arrays,\n",
        "        sampling_rate=feature_extractor.sampling_rate,\n",
        "        max_length=MAX_SEQ_LENGTH,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "    )\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "SEZuUTfI9Spc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we can feed the audio utterance samples to our model, we need to\n",
        "pre-process them. This is done by a Hugging Face Transformers \"Feature Extractor\"\n",
        "which will (as the name indicates) re-sample your the inputs to sampling rate\n",
        "the the model expects (in-case they exist with a different sampling rate), as well\n",
        "as generate the other inputs that model requires.\n",
        "\n",
        "To do all of this, we instantiate our `Feature Extractor` with the\n",
        "`AutoFeatureExtractor.from_pretrained`, which will ensure:\n",
        "\n",
        "We get a `Feature Extractor` that corresponds to the model architecture we want to use.\n",
        "We download the config that was used when pretraining this specific checkpoint.\n",
        "This will be cached so it's not downloaded again the next time we run the cell.\n",
        "\n",
        "The `from_pretrained()` method expects the name of a model from the Hugging Face Hub. This is\n",
        "exactly similar to `MODEL_CHECKPOINT` and we just pass that."
      ],
      "metadata": {
        "id": "X_qWJhJfA-W5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# getting audios\n",
        "audio_paths = df.iloc[:,4].to_list()\n",
        "\n",
        "def audio_converter(audio_paths):\n",
        "    # getting array of float samples composing each audio\n",
        "    audio_samples = list(map(read_wav_file, audio_paths))\n",
        "\n",
        "    print(f'Audios in the dataset: {len(audio_samples)}')\n",
        "\n",
        "    preproc_audio = preprocess_function(audio_samples)\n",
        "    sounds_converted  = { 'input_values': np.array(preproc_audio['input_values']),'attention_mask':np.array(preproc_audio['attention_mask'])}\n",
        "\n",
        "    return sounds_converted\n",
        "\n",
        "X = audio_converter(audio_paths=audio_paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46U9RBVRJgoE",
        "outputId": "966a09b7-d537-436c-d807-b1c6ff6da880"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audios in the dataset: 10039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting text labels"
      ],
      "metadata": {
        "id": "yV1LwzqoELJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_dict= {'ang':0, 'dis':1, 'exc':2, 'fea':3, 'fru':4, 'hap':5,'neu':6, 'oth':7, 'sad':8, 'sur':9, 'xxx':10}\n",
        "\n",
        "y = df.emotion.map(encoded_dict).to_numpy()"
      ],
      "metadata": {
        "id": "g1b5p3EL9v2X"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Labeled rows: {len(y)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYjw2XkzHl_a",
        "outputId": "b438acb1-17f3-416f-f077-a610bd38c75a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labeled rows: 10039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Splitting"
      ],
      "metadata": {
        "id": "bfRQBM-FL4Wn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splits = np.array([0.8,0.1,0.1])\n",
        "\n",
        "def split_dataset(X, y, splits):\n",
        "    \n",
        "    if np.sum(splits) != 1:\n",
        "      raise Exception ('Splits division is not valid, their sum must be == 1')\n",
        "    \n",
        "    if len(X) != len(y):\n",
        "      raise Exception ('X and y must have same number of rows')\n",
        "    \n",
        "\n",
        "\n",
        "    n_split_train_val = int(splits[0] * len(y))\n",
        "\n",
        "    n_split_val_test = int((splits[0]+splits[1]) * len(y))\n",
        "\n",
        "    train_X, train_y = (X[:n_split_train_val], y[:n_split_train_val])\n",
        "    val_X, val_y = (X[n_split_train_val:n_split_val_test], y[n_split_train_val:n_split_val_test])\n",
        "    test_X, test_y = (X[n_split_val_test:], y[n_split_val_test:])\n",
        "\n",
        "return train_X, train_y, val_X, val_y, test_X, test_y\n",
        "\n",
        "train_X, train_y, val_X, val_y, test_X, test_y =  split_dataset(X=X, y=y, splits=splits)\n",
        "\n",
        "assert len(X_train) == len(y_train)\n",
        "assert len(X_val) == len(y_val)\n",
        "assert len(X_test) == len(y_test)\n",
        "assert len(X_train)+len(X_val)+len(X_test) == len(X)\n",
        "assert len(y_train)+len(y_val)+len(y_test) == len(y)\n",
        "\n",
        "print('Train dimensions: ', len(X_train), len(y_train))\n",
        "print('Val dimensions: ', len(X_val), len(y_val))\n",
        "print('Test dimensions:', len(X_test), len(y_test) )"
      ],
      "metadata": {
        "id": "GvBD2AJFL3kX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdV0-BvgkwXR"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zShNluEykwXS"
      },
      "source": [
        "### Installing the requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "K181ixe_kwXV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fbc8281-e32d-45a2-8aeb-1ed557ab0231"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-_mv6i6ha\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-_mv6i6ha\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit cf11493dce0a1d22446efe0d6c4ade02fd928e50\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0.dev0) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0.dev0) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0.dev0) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0.dev0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0.dev0) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0.dev0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0.dev0) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0.dev0) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0.dev0) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0.dev0) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.0.dev0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.0.dev0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.0.dev0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.0.dev0) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (0.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (3.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2023.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (23.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install huggingface-hub\n",
        "!pip install joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv02H4ZdkwXY"
      },
      "source": [
        "### Importing the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "5Fa3teaqkwXZ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Only log error messages\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "# Set random seed\n",
        "tf.keras.utils.set_random_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xum6vkoBkwXa"
      },
      "source": [
        "### Define certain variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK5kTlg-kwXp"
      },
      "source": [
        "## Defining the Wav2Vec 2.0 with Classification-Head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-B2SkmskwXq"
      },
      "source": [
        "We now define our model. To be precise, we define a Wav2Vec 2.0 model and add a\n",
        "Classification-Head on top to output a probability ditribution of all classes for each\n",
        "input audio sample. Since the model might get complex we first define the Wav2Vec\n",
        "2.0 model with Classification-Head as a Keras layer and then build the model using that.\n",
        "\n",
        "We instantiate our main Wav2Vec 2.0 model using the `TFWav2Vec2Model` class. This will\n",
        "instantiate a model which will output 768 or 1024 dimensional embeddings according to\n",
        "the config you choose (BASE or LARGE). The `from_pretrained()` additionally helps you\n",
        "load pre-trained weights from the Hugging Face Model Hub. It will download the pre-trained weights\n",
        "together with the config corresponding to the name of the model you have mentioned when\n",
        "calling the method. For our task, we choose the BASE variant of the model that has\n",
        "just been pre-trained, since we fine-tune over it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "dvwv5gt5kwXq"
      },
      "outputs": [],
      "source": [
        "from transformers import TFWav2Vec2Model\n",
        "\n",
        "\n",
        "def mean_pool(hidden_states, feature_lengths):\n",
        "    attenion_mask = tf.sequence_mask(\n",
        "        feature_lengths, maxlen=MAX_FRAMES, dtype=tf.dtypes.int64\n",
        "    )\n",
        "    padding_mask = tf.cast(\n",
        "        tf.reverse(tf.cumsum(tf.reverse(attenion_mask, [-1]), -1), [-1]),\n",
        "        dtype=tf.dtypes.bool,\n",
        "    )\n",
        "    hidden_states = tf.where(\n",
        "        tf.broadcast_to(\n",
        "            tf.expand_dims(~padding_mask, -1), (BATCH_SIZE, MAX_FRAMES, HIDDEN_DIM)\n",
        "        ),\n",
        "        0.0,\n",
        "        hidden_states,\n",
        "    )\n",
        "    pooled_state = tf.math.reduce_sum(hidden_states, axis=1) / tf.reshape(\n",
        "        tf.math.reduce_sum(tf.cast(padding_mask, dtype=tf.dtypes.float32), axis=1),\n",
        "        [-1, 1],\n",
        "    )\n",
        "    return pooled_state\n",
        "\n",
        "\n",
        "class TFWav2Vec2ForAudioClassification(layers.Layer):\n",
        "    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n",
        "\n",
        "    def __init__(self, model_checkpoint, num_classes):\n",
        "        super().__init__()\n",
        "        # Instantiate the Wav2Vec 2.0 model without the Classification-Head\n",
        "        self.wav2vec2 = TFWav2Vec2Model.from_pretrained(\n",
        "            model_checkpoint, apply_spec_augment=False, from_pt=True\n",
        "        )\n",
        "        self.pooling = layers.GlobalAveragePooling1D()\n",
        "        # Drop-out layer before the final Classification-Head\n",
        "        self.intermediate_layer_dropout = layers.Dropout(0.5)\n",
        "        # Classification-Head\n",
        "        self.final_layer = layers.Dense(num_classes, activation=\"softmax\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # We take only the first output in the returned dictionary corresponding to the\n",
        "        # output of the last layer of Wav2vec 2.0\n",
        "        hidden_states = self.wav2vec2(inputs[\"input_values\"])[0]\n",
        "\n",
        "        # If attention mask does exist then mean-pool only un-masked output frames\n",
        "        if tf.is_tensor(inputs[\"attention_mask\"]):\n",
        "            # Get the length of each audio input by summing up the attention_mask\n",
        "            # (attention_mask = (BATCH_SIZE x MAX_SEQ_LENGTH) ∈ {1,0})\n",
        "            audio_lengths = tf.cumsum(inputs[\"attention_mask\"], -1)[:, -1]\n",
        "            # Get the number of Wav2Vec 2.0 output frames for each corresponding audio input\n",
        "            # length\n",
        "            feature_lengths = self.wav2vec2.wav2vec2._get_feat_extract_output_lengths(\n",
        "                audio_lengths\n",
        "            )\n",
        "            pooled_state = mean_pool(hidden_states, feature_lengths)\n",
        "        # If attention mask does not exist then mean-pool only all output frames\n",
        "        else:\n",
        "            pooled_state = self.pooling(hidden_states)\n",
        "\n",
        "        intermediate_state = self.intermediate_layer_dropout(pooled_state)\n",
        "        final_state = self.final_layer(intermediate_state)\n",
        "\n",
        "        return final_state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG7tm0mukwXs"
      },
      "source": [
        "## Building and Compiling the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPZfDFBSkwXs"
      },
      "source": [
        "We now build and compile our model. We use the `SparseCategoricalCrossentropy`\n",
        "to train our model since it is a classification task. Following much of literature\n",
        "we evaluate our model on the `accuracy` metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "dFUXL-oUkwXs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa38ffd4-abc8-49c0-d775-db04193329f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "\n",
            "TFWav2Vec2Model has backpropagation operations that are NOT supported on CPU. If you wish to train/fine-tine this model, you need a GPU or a TPU\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFWav2Vec2Model: ['project_q.bias', 'quantizer.weight_proj.weight', 'quantizer.weight_proj.bias', 'quantizer.codevectors', 'project_q.weight', 'project_hid.bias', 'project_hid.weight']\n",
            "- This IS expected if you are initializing TFWav2Vec2Model from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFWav2Vec2Model from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFWav2Vec2Model were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFWav2Vec2Model for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def build_model():\n",
        "    # Model's input\n",
        "    inputs = {\n",
        "        \"input_values\": tf.keras.Input(shape=(MAX_SEQ_LENGTH,), dtype=\"float32\"),\n",
        "        \"attention_mask\": tf.keras.Input(shape=(MAX_SEQ_LENGTH,), dtype=\"int32\"),\n",
        "    }\n",
        "    # Instantiate the Wav2Vec 2.0 model with Classification-Head using the desired\n",
        "    # pre-trained checkpoint\n",
        "    wav2vec2_model = TFWav2Vec2ForAudioClassification(MODEL_CHECKPOINT, NUM_CLASSES)(\n",
        "        inputs\n",
        "    )\n",
        "    # Model\n",
        "    model = tf.keras.Model(inputs, wav2vec2_model)\n",
        "    # Loss\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "    # Optimizer\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=1e-5)\n",
        "    # Compile and return\n",
        "    model.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "model = build_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vpxl-9HhkwXt"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LmcNTJPFw5O",
        "outputId": "044cde02-eddc-43a1-c5e3-bb0589a7e522"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6 4 6 ... 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "cTkPEcibkwXv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "4a5d7471-b6b4-4c78-f3bc-bb1b5535c8de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "10039/10039 [==============================] - 586s 54ms/step - loss: 1.9755 - accuracy: 0.2066 - val_loss: 1.9241 - val_accuracy: 0.2398\n",
            "Epoch 2/2\n",
            " 8181/10039 [=======================>......] - ETA: 1:25 - loss: 1.9440 - accuracy: 0.2226"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-f54a7668b513>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(X,y,\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_EPOCHS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m ) #train_x,train[\"label\"],(test_x, test[\"label\"])\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model.fit(X,y,\n",
        "    validation_data=(X,y),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=MAX_EPOCHS\n",
        ") #train_x,train[\"label\"],(test_x, test[\"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n",
        "tf.keras.utils.plot_model(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "iRElbcWfJ0td",
        "outputId": "2ae5e983-9d4e-4164-b48e-9f63c3af9eaa"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)           [(None, 16000)]      0           []                               \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 16000)]      0           []                               \n",
            "                                                                                                  \n",
            " tf_wav2_vec2_for_audio_classif  (None, 11)          94380171    ['input_4[0][0]',                \n",
            " ication_1 (TFWav2Vec2ForAudioC                                   'input_3[0][0]']                \n",
            " lassification)                                                                                   \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 94,380,171\n",
            "Trainable params: 94,380,171\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNhmUa-SkwXv"
      },
      "source": [
        "Great! Now that we have trained our model, we predict the classes\n",
        "for audio samples in the test set using the `model.predict()` method! We see\n",
        "the model predictions are not that great as it has been trained on a very small\n",
        "number of samples for just 1 epoch. For best results, we reccomend training on\n",
        "the complete dataset for at least 5 epochs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnclv5ILkwXv"
      },
      "outputs": [],
      "source": [
        "preds = model.predict(test_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxY9aj3lkwXw"
      },
      "source": [
        "Now we try to infer the model we trained on a randomly sampled audio file.\n",
        "We hear the audio file and then also see how well our model was able to predict!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knY3vq2WkwXw"
      },
      "outputs": [],
      "source": [
        "import IPython.display as ipd\n",
        "\n",
        "rand_int = random.randint(0, len(test_x))\n",
        "\n",
        "ipd.Audio(data=np.asarray(test_x[\"input_values\"][rand_int]), autoplay=True, rate=16000)\n",
        "\n",
        "print(\"Original Label is \", id2label[str(test[\"label\"][rand_int])])\n",
        "print(\"Predicted Label is \", id2label[str(np.argmax((preds[rand_int])))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sp5sXK4EkwXx"
      },
      "source": [
        "Now you can push this model to Hugging Face Model Hub and also share it with with all your friends,\n",
        "family, favorite pets: they can all load it with the identifier\n",
        "`\"your-username/the-name-you-picked\"`, for instance:\n",
        "\n",
        "```python\n",
        "model.push_to_hub(\"wav2vec2-ks\", organization=\"keras-io\")\n",
        "tokenizer.push_to_hub(\"wav2vec2-ks\", organization=\"keras-io\")\n",
        "```\n",
        "And after you push your model this is how you can load it in the future!\n",
        "\n",
        "```python\n",
        "from transformers import TFWav2Vec2Model\n",
        "\n",
        "model = TFWav2Vec2Model.from_pretrained(\"your-username/my-awesome-model\", from_pt=True)\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('derc': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "c95121c4820229e40183c12cf809add4657f5d8cdc20fb3d24f9d7a7ade02504"
      }
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}